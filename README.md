# **RAG System for Querying Fairness & Bias Research in LLMs**

A domain-specialized Retrieval-Augmented Generation (RAG) system designed to answer technical questions about *fairness and bias in Large Language Models (LLMs)*.
The system indexes 20+ research papers, retrieves relevant context, and generates grounded, citation-backed answers using FLAN-T5-XL.

This repository contains the full pipeline:

â¡ï¸ **PDF processing â†’ chunking â†’ embedding â†’ FAISS retrieval â†’ FLAN-T5 generation â†’ evaluation â†’ UI**

---

## **ğŸš€ Features**

* **Sentence-level chunking** using spaCy transformers
* **Three embedding models** supported: MiniLM, BGE-small, E5-base
* **FAISS vector search** with inner-product similarity
* **Context-aware generation** using FLAN-T5-XL
* **Caching** for repeated queries
* **Human & automatic evaluation framework** (Precision@K, Recall@K, MRR, ROUGE)
* **Gradio UI** for interactive querying
* **124 custom annotated Q/A pairs** for evaluation

---

## **ğŸ“ Repository Structure**

```
rag-llm-bias-research/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prepare_data.py        # PDF extraction, sentence chunking, filtering
â”‚   â”œâ”€â”€ retrieval.py           # E5-based retrieval, caching, FAISS search
â”‚   â”œâ”€â”€ generation.py          # FLAN-T5-XL answer generation
â”‚   â”œâ”€â”€ evaluation.py          # Precision@K, Recall@K, MRR, ROUGE, human eval
â”‚   â”œâ”€â”€ ui.py                  # Gradio interface
â”‚   â”œâ”€â”€ config.py              # All system-wide configuration flags
â”‚   â””â”€â”€ main.py                # Entry point for running modules
â”‚
â”œâ”€â”€ papers_pdf/                # 20+ research papers (PDF)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chunks.jsonl           # 2,847 processed chunks
â”‚   â”œâ”€â”€ metas.jsonl            # Chunk metadata
â”‚   â”œâ”€â”€ embeddings.npy         # Embeddings matrix
â”‚   â”œâ”€â”€ version.json           # Index versioning info
â”‚
â”‚
â”œâ”€â”€ index/
â”‚   â””â”€â”€ faiss_index.idx        # FAISS vector index
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluation_results.json
â”‚   â”œâ”€â”€ ablation_study.json
â”‚   â”œâ”€â”€ error_analysis.json
â”‚   â”œâ”€â”€ ablation_comparison.png
â”‚   â”œâ”€â”€ evaluation_results.png
â”‚   â””â”€â”€ human_eval.csv
â”‚
â”œâ”€â”€ qa_pairs.json          # 124 annotated Q/A pairs
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## **ğŸ›  Installation**

### **1. Clone the repository**

```bash
git clone https://github.com/NorthChat/NLP_Final_Project.git
cd NLP_Final_Project
```

### **2. Install dependencies**

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126
pip install -r requirements.txt
```

### **3. (Optional) Download spaCy model**

```bash
python -m spacy download en_core_web_trf
```

---

## **ğŸ“š Preparing the Dataset**

### **Extract PDFs, chunk text, create embeddings & build index**

This runs the entire preprocessing pipeline:

```bash
python src/main.py --prepare --model e5-base
```

This will:

* Extract clean text from PDFs
* Create 5-sentence overlapping chunks
* Filter low-quality chunks (tables, captions, citations)
* Generate embeddings
* Build FAISS index

You can also run ablation settings:

```bash
python src/main.py --prepare --ablation
```

---

## **ğŸ” Running Retrieval + Generation**

### **Command-line RAG run**

```bash
python src/main.py --query "How does Auto-Debias detect biased prompts?"
```

Outputs:

* Retrieved chunks
* Model-generated answer
* Paper IDs used

---

## **ğŸ§ª Evaluation**

### **Automatic evaluation (Precision@K, Recall@K, MRR, ROUGE):**

```bash
python src/main.py --evaluate
```

![Evaluation Results](evaluation/evaluation_results.png)

### **Human evaluation workflow:**

Human eval covers:

* Correctness (1â€“5)
* Groundedness (1â€“5)
* Completeness (1â€“5)

---

## **ğŸŒ Launch the Interactive UI**

```bash
python src/main.py --ui
```

The interface supports:

* Live questioning
* Adjustable Top-K retrieval
* Retrieved context display
* Answer grounding transparency

Runs locally at:

```
http://localhost:7860
```

---

## **ğŸ§ª Ablation Study**

The project includes experiments comparing:

| Model         | Dim | Retrieval P@5 | ROUGE-L   |
| ------------- | --- | ------------- | --------- |
| **BGE-small** | 384 | **0.160**     | 0.131     |
| **E5-base**   | 768 | 0.156         | **0.154** |
| MiniLM-L6     | 384 | 0.155         | 0.111     |

![Ablation Comparison](evaluation/ablation_comparison.png)

Run study:

```bash
python src/main.py --prepare --ablation
```

---

## **ğŸ“Š Dataset Details**

### **Papers indexed:** 20+

### **Chunks created:** 2,847 (sentence-level, filtered)

### **Evaluation Q/A pairs:** 124

---

## **ğŸ§  System Architecture**

```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚      PDF Corpus       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            extract + clean text (PyMuPDF)
                          â”‚
               sentence segmentation (spaCy)
                          â”‚
           chunk filtering + deduplication
                          â”‚
                  embeddings (E5/BGE/MiniLM)
                          â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    FAISS Vector Index   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ retrieve top-K
                          â–¼
                context selection & packing
                          â”‚
                 generate answer using
                    FLAN-T5-XL
                          â–¼
                  grounded final answer
```

---

## **ğŸ“Œ Configuration**

All config variables are defined in **`src/config.py`**:

* `CHUNK_SIZE`
* `CHUNK_OVERLAP`
* `TOP_K`
* `EMBEDDING_MODEL`
* `USE_CACHING`
* `MAX_NEW_TOKENS`
* `RETRIEVAL_MODEL`

Edit these to change behavior.

---

## **ğŸ‘©â€ğŸ’» Contributors**

### **Harini Hari**

* Data preparation pipeline
* Human evaluation framework
* Error analysis & visualizations
* Gradio UI
* 62 Q/A annotations

### **Preeta Chatterjee**

* Retrieval module (E5 support, caching, expansion)
* Generation module (FLAN-T5-XL, prompting)
* Metrics framework (P@K, R@K, MRR, ROUGE)
* Chunk filtering & index versioning
* Ablation experiments
* 62 Q/A annotations

---

## **ğŸ“ Citation**

If you use or reference this work:

```
Hari, H., & Chatterjee, P. (2025). 
Retrieval-Augmented Generation for Summarizing and Querying Research 
on Fairness and Bias in LLMs.
```

---

## **ğŸ“¬ Contact**

For any questions or issues:
**Preeta Chatterjee & Harini Hari**

