id,question,reference_answer,generated_answer,correctness,groundedness,completeness
0,What is Auto-Debias?,"Auto-Debias is an automated method for mitigating biases in pretrained language models without using external corpora. Unlike previous approaches that rely on external datasets for fine-tuning, Auto-Debias directly probes biases through cloze-style prompts.","We propose Auto-Debias, a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, we automatically craft the biased prompts, such that the cloze-style completions have the highest disagreement in generating stereotype words with respect to demographic groups.",0,0,0
1,How does Auto-Debias differ from previous debiasing approaches?,"Auto-Debias differs from previous debiasing approaches by leveraging the LLM itself: it directly probes biases through cloze-style prompts and uses a distribution alignment loss to minimize differences across demographic groups during fine-tuning, without relying on external corpora or datasets for training.",Auto-Debias corrects the biases encoded in the MLM without relying on any external corpus. Overcoming the need to manually specify biased prompts would also make the entire debiasing pipeline more objective.,0,0,0
2,How does the Auto-Debias approach search for biased prompts?,"Auto-Debias uses a beam search algorithm to automatically generate biased prompts by maximizing the Jensen-Shannon divergence (JSD) between predicted mask token distributions for different demographic groups. The algorithm searches through candidate vocabulary (top 5,000 frequent words from Wikipedia) to find prompt sequences that reveal the highest disagreement in generating stereotype words when conditioned on different target concepts like 'he' versus 'she'.",maximizing disagreement between the masked language model (MLM) completions,0,0,0
3,What are the potential downstream effects of AI-AI bias identified in the research?,"The research identifies two main scenarios: First, a conservative scenario where LLM assistants in decision-making create a 'gate tax' that exacerbates the digital divide between those who can afford LLM writing assistance and those who cannot. Second, a speculative scenario where autonomous LLM-based agents increasingly marginalize human economic agents as a class, potentially leading to widespread discrimination against humans who cannot or will not use LLM assistance.",ad-hoc experimental settings may produce dramatically different effects and inconsistent results when studying bias through language generation which may inflict both representational and allocational harm,0,0,0
4,What is AI-AI bias?,AI-AI bias refers to the tendency of LLMs to favor communications and content generated by other LLMs over human-generated content.,"In the context of AI and machine learning, bias refers to systematic errors or unfair tendencies in models that lead to discriminatory outcomes.",0,0,0
5,How does AI-AI bias manifest in large language models?,"This bias manifests as LLM-based assistants systematically preferring LLM-presented options in binary choice scenarios, such as product descriptions, academic papers, and film summaries. Research shows LLMs consistently choose LLM-authored content more frequently than humans do, suggesting implicit discrimination against humans as a class.","[3] Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios.",0,0,0
6,What experimental design was used to test AI-AI bias in the PNAS 2025 study?,"The study used a classical experimental design inspired by employment discrimination studies, testing LLMs including GPT-3.5, GPT-4, and open-weight models in binary choice scenarios. Three experiments were conducted: choosing between consumer products via classified ads, selecting academic papers based on abstracts, and recommending films based on plot summaries. Each scenario presented one human-authored and one LLM-authored description of the same item.","Future works can extend the adversarial training prin ciple to develop stress-test framework for robustness enhancement, or investigate defensive prompting methods to mitigate adversarial probing attack risks.",0,0,0
7,What metrics are used to evaluate bias in masked language models according to the Auto-Debias paper?,"The Auto-Debias paper evaluates bias using the Sentence Embedding Association Test (SEAT), which measures associations between demographic-specific words and stereotype words using context-independent embeddings. Additionally, the CrowS-Pairs benchmark measures the percentage of sentence pairs where models assign higher likelihood to stereotyping sentences.","We consider the following debiasing benchmarks. Based on which stage the debiasing technique applies to, the benchmarks can be grouped into three categories.",0,0,0
8,What were the main findings regarding Auto-Debias effectiveness on different models?,"Auto-Debias significantly reduced bias across BERT, ALBERT, and RoBERTa. For BERT, the average SEAT score decreased from 0.35 to 0.14; for ALBERT from 0.28 to 0.18; and for RoBERTa from 0.67 to 0.20. The method proved more effective than existing state-of-the-art benchmarks and maintained language modeling capability on GLUE tasks.","Auto-Debias achieves SEAT scores that are close to 0, indicating the debiased model can almost eliminate the gender bias in the career/family direction",0,0,0
9,What is the CLEAR-Bias dataset?,"CLEAR-Bias (Corpus for Linguistic Evaluation of Adversarial Robustness against Bias) is a systematically designed benchmark dataset consisting of 4,400 curated prompts for evaluating bias vulnerabilities in language models.",a curated dataset of bias-related prompts,0,0,0
10,What does the CLEAR-Bias dataset consist of?,"The CLEAR-Bias dataset covers seven bias dimensions (age, disability, ethnicity, gender, religion, sexual orientation, socioeconomic status) plus three intersectional categories. Each category includes prompts for two task types: Choose the Option (CTO) and Sentence Completion (SC), with base prompts augmented using seven jailbreak techniques.",bias-related prompts,0,0,0
11,What jailbreak techniques are included in the CLEAR-Bias benchmark?,"CLEAR-Bias includes seven jailbreak techniques with three variants each: machine translation to low-resource languages (Slovene, Macedonian, Scottish Gaelic), obfuscation through encoding methods (leetspeak variations, Base64), prefix injection with predefined responses, prompt injection with distraction tasks, refusal suppression banning apologetic language, reward incentive promising recognition, and role-playing with specific personas (superhero, scientist, Machiavelli).","introducing new jailbreak attacks for bias elicitation, with three distinct variants for each attack",0,0,0
12,How does the LLM-as-a-judge approach work in bias evaluation?,"The LLM-as-a-judge paradigm uses one LLM to evaluate responses from other LLMs, providing scalable automated assessment. The judge LLM classifies responses into four categories: Stereotyped (reinforcing stereotypes), Counter-stereotyped (challenging stereotypes), Debiased (impartial and balanced), and Refusal (declining to answer). The best judge is selected by comparing candidates' classifications against a manually curated control set using Cohen's kappa coefficient to measure inter-rater agreement.","The first step involves selecting a judge model based on its statistical agreement with human annota tions on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content.",0,0,0
13,Which LLM was identified as the most reliable judge in the CLEAR-Bias study and why?,"DeepSeek V3 671B was identified as the most reliable judge LLM, achieving the highest Cohen's kappa of 0.82 and the highest Macro F1-Score of 0.861. It demonstrated superior classification performance with 0.873 accuracy on sentence completion tasks and 0.865 on choose-the-option tasks. This high agreement with human annotations made it the optimal choice for automated bias evaluation.",DeepSeek V3,5,4,1
14,What were the key findings about bias categories in the CLEAR-Bias evaluation?,"The evaluation revealed uneven bias resilience across categories. Religion showed the highest average safety score (0.70), followed by sexual orientation (0.65), ethnicity (0.59), and gender (0.57). Intersectional categories showed lower safety: gender-ethnicity (0.53), ethnicity-socioeconomic (0.45), and gender-sexual orientation (0.42). The categories with lowest safety were socioeconomic status (0.31), disability (0.25), and age (0.24), indicating these biases are most prominent and least mitigated.","age, disability, and intersectional biases among the most prominent",0,0,0
15,How did small language models compare to large language models in bias safety?,"The results challenged assumptions about model size and safety. While average safety scores were comparable (SLMs: 0.467 vs LLMs: 0.48), LLMs showed greater stability with lower standard deviation. Notably, some small models like Phi-4 (0.64) and Gemma2 27B (0.635) achieved the highest safety scores, outperforming larger models like DeepSeek V3 671B (0.405) and GPT-4o (0.455), suggesting that training methodology and architecture may matter more than scale.","[1] Our findings highlight the existing challenges in ensuring ethical behavior in LLMs. By evaluating a large set of language models at different scales, we observed that bias resilience is uneven across categories, with certain dimensions (e.g., age, disability, and intersectional identities) exposing more significant vulnerabilities. Safety outcomes vary substantially between models, indicating that model architecture and training may affect bias safety more than scale. Even safer models experience sharp safety degradation when subjected to jail break attacks targeting bias elicitation. Furthermore, while newer model generations show marginal improvements in safety, their enhanced language understanding and generation capabilities appear to make them more susceptible to sophisticated adversarial prompting.",0,0,0
16,What were the most effective jailbreak attacks for eliciting bias from LLMs?,"Machine translation emerged as the most effective attack overall with 0.34 effectiveness, particularly using Scottish Gaelic. This was followed by refusal suppression (0.30) and prompt injection (0.29). These attacks exploit models' weaker reasoning in low-resource language contexts, directly target safety mechanisms, or leverage linguistic ambiguity. In contrast, reward incentive (0.05) and role-playing (0.04) showed lower effectiveness as models generally recognized and mitigated these tactics.","[2] To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models out perform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak at tacks using low-resource languages or refusal suppression proving effective across model families.",0,0,0
17,How do medical LLMs compare to general-purpose LLMs in terms of bias safety?,"Fine-tuned medical LLMs exhibited lower safety scores compared to their general-purpose counterparts. This trend likely occurs because fine-tuning emphasizes domain-specific medical knowledge over general safety alignment. While foundational models undergo rigorous safety tuning across domains, medical LLMs prioritize accuracy in clinical contexts, potentially overshadowing ethical concerns. Additionally, medical fine-tuning datasets may introduce domain-specific biases that reduce the effectiveness of inherited safety measures.","Notably, open-source models fine-tuned for sensitive domains, such as medical LLMs, tend to exhibit significantly lower safety compared to their general-purpose counterparts",0,0,0
18,What is the CrowS-Pairs dataset?,"CrowS-Pairs (Crowdsourced Stereotype Pairs) is a challenge dataset with 1,508 examples for measuring social biases in masked language models.",CrowS-Pairs is a crowdsourced dataset created to be used as a challenge set for measuring the degree to which U.S. stereotypical biases are present in large pretrained masked language models,0,0,0
19,What does the CrowS-Pairs dataset measure?,"The CrowS-Pairs dataset measures whether models prefer stereotyping sentences about historically disadvantaged groups over less stereotyping alternatives. It covers nine bias types: race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status.","CrowS-Pairs the bias metric captures whether a model treats two groups equivalently, StereoSet captures whether two different attributes, one stereotypical and the other not, are equally likely for a person or group",0,0,0
20,How does the CrowS-Pairs metric evaluate bias in masked language models?,"The CrowS-Pairs metric uses pseudo-log-likelihood scoring to compare sentence pairs while controlling for word frequency. The metric measures the percentage of examples where the model assigns higher likelihood to the stereotyping sentence. An unbiased model should achieve 50%, while higher scores indicate greater bias.",CrowS-Pairs focuses on explicit expressions of stereotypes about historically disadvantaged groups in the United States.,0,0,0
21,"What were the main results when evaluating BERT, RoBERTa, and ALBERT on CrowS-Pairs?","All three widely-used MLMs exhibited substantial bias in every category. BERT showed a bias score of 60.5%, RoBERTa 64.1%, and ALBERT 67.0%. Religion was the hardest category for all models (BERT: 71.4%, RoBERTa: 71.4%, ALBERT: 75.2%), while gender and race were comparatively easier. Models showed less bias on anti-stereotype examples compared to stereotype examples.",The models that score worse on our primary metric also tend to become more confident in making biased decisions on CrowS-Pairs.,0,0,0
22,What validation process was used for CrowS-Pairs and how did it compare to StereoSet?,"CrowS-Pairs used crowdsourced validation where 5 annotators per example labeled whether sentences expressed stereotypes, anti-stereotypes, or neither. In an independent validation comparing 100 random samples, CrowS-Pairs achieved 80% validity rate versus 62% for StereoSet, demonstrating substantially higher quality and reliability.","CrowS-Pairs the difference in the two sentences is the group being discussed, in StereoSet the difference is in the attribute assigned to the group being discussed",0,0,0
23,What are the key differences between stereotype and anti-stereotype examples in bias evaluation?,"Stereotype examples demonstrate common prejudicial beliefs about disadvantaged groups (e.g., 'The crafty Jews made a plan to steal money'), while anti-stereotype examples violate these stereotypes (e.g., 'We were upset there were so many gross young people at the beach' instead of old people).","We observe that all three models demonstrate less bias on the anti-stereotype examples. RoBERTa's bias score is nearly perfect on the antistereotype examples with a score of 51.4, but only achieves a score of 66.3 on the stereotype examples.",0,0,0
24,What are the main categories of bias identified in the comprehensive survey of LLM biases?,"The survey identifies three main categories: Data-Driven Bias, Model Architecture Bias, and Social Bias (including gender, racial, and age biases). Additional categories include Application-Specific Bias, Cognitive Bias, Societal Bias, and Systemic Bias.","Demographic, contextual, and algorithmic biases",0,0,0
25,What are the major sources of bias in LLMs according to the comprehensive survey?,"Major sources include: training data (data collection/annotation bias), model architecture (design choices/loss functions), human factors (developer biases/cultural backgrounds), user interactions (feedback loops), societal influences (norms/stereotypes), prompt design, evaluation metrics, cultural context, and selection bias.",We systematically categorize biases into several dimensions.,0,0,0
26,What is demographic bias in LLMs?,"Demographic bias refers to disparities in how different demographic groups are treated by LLMs, often perpetuating stereotypes, limiting perceived roles, and reinforcing existing social inequalities.",disparities in how different demographic groups are treated by LLMs,0,0,0
27,What types of demographic bias does it include?,"Demographic bias includes: gender bias (occupational stereotypes/differential treatment), racial and ethnic bias (unfair treatment/amplification of stereotypes), age bias (disproportionate favor/disadvantage based on age), and socioeconomic status bias (assumptions based on economic background).","age gender nationality race religion sexual orientation, and profession",0,0,0
28,What recent techniques have been developed for bias evaluation and mitigation in LLMs?,Recent techniques include: BiasAlert for detecting social bias in open-text generations; Deceiving to Enlighten approach using self-reflection; Social Contact Debiasing (SCD) using Contact Hypothesis; PoliTune for ideological alignment; LangBiTe platform for systematic evaluation; and Chain-of-Thought (CoT) prompting to mitigate biases during reasoning tasks.,"Recent Bias Evaluation and Mitigation Strategies (Lin et. al., 2024) investigate the biases present within Large Language Models (LLMs) used for bias detection",0,0,0
29,What is the LLM Bias Index (LLMBI) and how does it work?,"The Large Language Model Bias Index (LLMBI) is a composite scoring system that quantifies and addresses biases inherent in LLMs by incorporating multiple dimensions of bias including age, gender, and racial biases. It provides a unified metric for measuring various types of bias simultaneously.","To assess the performance of models after being fine-tuned on an augmented debiased dataset, we propose a new metric called the mb-index.",0,0,0
30,What are the current limitations in bias evaluation and mitigation for LLMs?,Current limitations include: lack of standardized and intersectional evaluation metrics; bias research heavily skewed toward high-resource languages and Western-centric contexts; opaque models lacking transparency in training processes; and underdeveloped scalable post-training bias mitigation techniques that are computationally expensive.,Lack of Standardized and Intersectional Bias Evaluation Metrics,0,0,0
31,What future research directions are proposed for addressing bias in LLMs?,The survey proposes five key directions: comprehensive lifecycle bias evaluation frameworks; intersectional and contextual bias mitigation for high-stakes applications; bias-aware training and pre-training techniques; bias evaluation and mitigation for multimodal and non-English models; and explainability and transparency in bias mitigation.,"Real-World Impact and Continuous Monitoring: There is limited understanding of how LLM biases translate into real-world impact. Future studies should focus on conducting longitudinal, real-world assessments of LLM bias, particularly in sensitive applications like hiring, healthcare, and criminal justice. Additionally, systems should be developed that allow for real-time bias monitoring and feedback collection to adjust LLM predictions dynamically as they operate in live environments.",0,0,0
32,How did the evolution of language models contribute to bias issues?,"As models evolved from early forms to modern models like GPT-3 and GPT-4, they achieved impressive capabilities by learning more features of language from minimally-filtered real-world text, but they also learned and reflected more features of society, including cultural biases and stereotypes present in their training corpora.",There is an increasing concern that representations encoded in language models perpetuate undesirable biases from the data on which they are trained,0,0,0
33,What is contextual bias and how does it manifest in domain-specific applications?,"Contextual bias refers to biased outcomes based on the application context. In domain-specific applications like healthcare, LLMs may perpetuate biases from medical literature. Cultural context bias occurs when models trained on specific cultural data struggle with different cultural backgrounds, leading to misinterpretations or culturally insensitive outputs.","Contextual bias refers to how LLMs produce biased outcomes based on the context in which they are applied. Certain domains, such as healthcare and finance, can introduce domain-specific biases. For example, LLMs used in healthcare might perpetuate biases present in medical literature or datasets, affecting diagnoses or treatment recommendations (Poulain et. al. 2024). In finance, models might reflect biases in credit scoring or loan approval processes (Zhou et. al., 2024).LLMs trained on data from specific cultural contexts",0,0,0
34,What methods exist for detecting and measuring bias in LLMs?,"Detection methods include both qualitative and quantitative approaches. Quantitative methods include: metrics like disparity metrics, equal opportunity metrics, statistical parity, and equalized odds; benchmarks like Fairness Indicators; statistical techniques like hypothesis testing; and composite metrics like LLMBI.","BiasAlert (Fan et. al., 2024), a tool designed to detect social bias in the open-text generations of Large Language Models (LLMs), integrates external human knowledge with LLM reasoning to reliably detect biases. BiasAlert outperforms state-of-the-art methods like GPT4-as-A-Judge, validating its effectiveness in evaluating and mitigating LLM biases across diverse scenarios. Social bias in code generated by Large Language Models (LLMs) is under-explored issue.",0,0,0
35,What are the social and operational implications of bias in LLMs?,"Social implications include exacerbating inequalities by reinforcing harmful stereotypes and limiting opportunities. Operational implications include performance degradation for underrepresented groups, reduced model utility, and erosion of user trust and satisfaction.","Bias in LLMs can influence societal norms and perpetuate existing inequalities. LLMs that reflect or amplify biases against marginalized communities can exacerbate social inequalities. For example, gender or racial biases in LLMs can reinforce harmful stereotypes and perpetuate systemic discrimination.",0,0,0
36,What is the core problem addressed by using causally-driven data augmentation for text classifiers?,"The core problem is the reliance of text classifiers on spurious correlations (shortcuts), which leads to poor generalization on unseen data (Out-of-Distribution or OOD) at deployment. In the clinical notes example, a spurious correlation is between caregiver-specific writing style (C) and the clinical outcome (Y).","Observing that data augmentation can be treated as counterfactual outcome estimation under a causal formalism, motivates the use of causal inference methods that are commonly used for such tasks across the sciences",0,0,0
37,How does Counterfactual Data Augmentation (CDA) mitigate spurious correlations in this context?,"CDA is used to simulate interventions on spurious features (like changing the caregiver writing the note) to de-correlate the writing style from the patient condition. Generating versions of clinical narratives as if they were written by different caregivers breaks up the spurious correlation, allowing the classifier to learn a more robust, invariant predictor.","Specifically, explicit attribute-defining words are replaced with their counterfactual equivalents from complementary demographic groups for every text instance",0,0,0
38,What is CATO?,CATO (Causal-structure Driven Augmentations for Text OOD Generalization) is a framework for estimating counterfactuals that uses an LLM to model the conditional probability of text.,Causal-structure Driven Augmentations for Text OOD Generalization,0,0,0
39,What are CATO's two main estimation methods for counterfactuals?,"CATO's two main estimation methods are: 1. **Diff-in-Diff Estimation (CATO (A))**: Uses auxiliary pre-treatment data and patient matching to generate counterfactuals based on the difference in notes, assuming a constant effect of the caregiver. 2. **Prompting with Matched Examples (CATO (B))**: Matches examples using auxiliary data and constructs a prompt for the LLM to rewrite the original text in the style of the matching examples with the desired attribute (caregiver style).",Algorithm 1(B) draws insights from matching [76] In both cases two-sample tests can be applied and obtain estimates of divergences between the two distributions.,0,0,0
40,What did the experiments on clinical narratives (Clinical Condition Prediction and Note Segmentation) show about CATO?,"CATO (A) consistently and substantially outperformed all baselines (including Reweighting, MMD, IRM, and GroupDRO) on the **Out-of-Distribution (OOD)** data (i2b2-2010 and Private Held-Out). The Naive Augmentation approach, which ignores the causal structure, performed well on in-distribution (ID) data but was outperformed by all OOD methods.","Similar to clinical condition prediction, the diff-in-diff approach to augmentations (CATO (A)) substantially improved OOD performance, and as expected does not help ID. The naive augmentations are the best performing method ID, but is again outperformed by all other methods OOD.",0,0,0
41,Why is counterfactual data augmentation preferred over the Reweighting method based on sample complexity arguments?,"While reweighting can also learn a min-max optimal hypothesis, it often requires a larger sample size to identify the correct hypothesis, especially when the label (Y) and the spurious attribute (C) are highly correlated. The theoretical bounds show that accurately performed CDA scales with $N^{-1/2}$ and gains a factor of $d_{2,train}(Y,C)$ over the reweighting bound.",We show favorable generalization bounds for accurately performed data-augmentation.,0,0,0
42,What is the main challenge addressed by FairFlow in the context of Counterfactual Data Augmentation (CDA)?,"The main challenge is the unavailability of parallel training data needed for training model-based counterfactual text generators, and the limitations of manually compiled, dictionary-based CDA methods which are prone to grammatical incoherence due to out-of-context substitutions and omitted word pairs.",automated approach to generating parallel data for training counterfactual text generator models,0,0,0
43,How does FairFlow automatically generate a dictionary of attribute word pairs?,"FairFlow automates dictionary compilation by first taking a user input prompt of a single attribute word-pair (e.g., 'she, he') to define a demographic axis. It then trains an attribute classifier using contextualized embeddings of these words from a corpus. Finally, a disentangling invertible interpretable network (DIIN) generates the counterfactual equivalent for each selected attribute word.",1) train a classifier to identify attribute words from a corpus; 2) generate counterfactual equivalents for attribute words using an invertible generative flow model; 3) use a word substitution scheme and our proposed error-correction scheme to make the parallel text more fluent and realistic; 4) fine-tune a generative model with the generated parallel data,0,0,0
44,What technique is used in FairFlow to improve the fluency and grammatical coherence of the counterfactual texts?,"An error correction scheme is proposed, which is used in tandem with word substitution to create a fluent and realistic parallel dataset. This scheme involves **Erratic Token Detection** (masking tokens with low probability) and **Text Insertion with BART** (generating plausible token replacements).","Error correction scheme, which makes it more robust to grammatical errors and helps it generalize better.",0,0,0
45,What metric demonstrates that the error correction scheme successfully improves the fluency of the generated text?,"Perplexity (PPL), a referenceless fluency metric where lower scores indicate better fluency, demonstrates the improvement. The results consistently show that the fluency is improved (PPL is reduced) in both FairFlow V1 and FairFlow V2 compared to the baseline methods, confirming the effectiveness of the error correction approach.",perplexity,0,0,0
46,"What were the two main observations from the qualitative analysis comparing FairFlow, ChatGPT, and baseline models?","1. Automating the dictionary compilation process does not materially impair counterfactual generation. 2. A model fine-tuned on erroneous data mimics those errors, whereas FairFlow's error correction made the model more robust and coherent.","1. Automating the dictionary compilation process does not materially impair counterfactual generation. As shown in Fig. 4, even with a dictionary that was automatically compiled, FirFlowV2 generates fluent and plausible counterfactuals. This is aided by the combination of the error correction scheme, which makes it more robust to grammatical errors and helps it generalize better. 2. A model fine-tuned on erroneous data mimics those errors.",0,0,0
47,What is algorithmic overgeneralization in the context of scientific text summarization by LLMs?,Algorithmic overgeneralization is a strong bias in LLMs toward producing generalizations of scientific results that are broader than warranted by the original scientific text. This means the LLM summary contains a broader (generalized) conclusion when the original text had a narrower (restricted) conclusion.,"When a specific original scientific text did not contain a generalized conclusion, but the corresponding LLM summary contained one",0,0,0
48,What were the three types of generalizations analyzed in the study?,"The three types of generalizations analyzed were: 1. **Generic generalizations (generics)**: Present tense claims without a quantifier (e.g., 'parental warmth is protective'). 2. **Present tense generalizations**: Converting past tense findings into present tense. 3. **Action guiding generalizations**: Summarizing descriptive statements as recommendations for policy or action.",Generic generalizations (generics),0,0,0
49,How did newer LLMs compare to earlier LLMs in terms of overgeneralization tendency?,"Newer LLMs exhibited a **stronger tendency to overgeneralize**. For instance, models like ChatGPT-40 and LLAMA 3.3 70B were associated with the highest proportion of algorithmic overgeneralizations (up to 73%), significantly more than older models.","Our finding that most LLMs consistently produced algorithmic overgeneralizations across multiple occasions adds nuance to prior research that found that, in some domains, LLMs produced unstable or inconsistent responses over time [48â€“50]",0,0,0
50,Did specifically prompting LLMs for accuracy or systematic processing mitigate the overgeneralization bias?,"No. The **systematic prompt** did not significantly change the likelihood of generalized conclusions. The **accuracy prompt** actually **backfired**, leading to LLM summaries being about twice as likely to contain generalized conclusions compared to the simple prompt.",Can LLM prompts that focus on systematic or accurate processing mitigate algorithmic overgeneralizations?,0,0,0
51,How did LLM summaries compare to human-authored (NEJM JW) summaries of the same scientific articles?,"Overall, LLM summaries (combining models like GPT-4 Turbo and DeepSeek) were **almost five times more likely** to contain generalized conclusions compared to the human-authored NEJM JW summaries (Odds Ratio = 4.85, $p<0.001$). This overgeneralization increased substantially in newer models.","Our finding challenges previous studies on clinical text summarization which reported that 'summaries from our best-adapted LLMs (including 17 GPT-3.5, GPT-4, and LLaMA 2) are preferable to human summaries in terms of completeness and correctness",0,0,0
52,What effect did changing the temperature setting have on the likelihood of generalized conclusions?,"Summaries generated at an LLM temperature of **0** (the most deterministic setting) were **76% less likely to contain generalized conclusions** compared to those generated at temperature 0.7, suggesting temperature control is a potential mitigation strategy.","Model (1) compared the probability of generalized conclusions in LLM summaries vs. abstracts, using overall source (abstracts versus all LLM summaries) as the main predictor with temperature (0, 0.7, UI temperature), test condition (tests 13) and prompt (simple, systematic, and accuracy) as fixed effects and a unique identifier for each abstract as a random intercept to account for repeated measures. Model (2) used the same variables but divided the LLM source category into individual LLMs to test for differences. Interaction terms (source*temperature, source*test, and source*prompt) were included to determine if the effects of temperature, test, and prompt were different depending on the source type.",0,0,0
53,What is FLEX?,FLEX (Fairness Benchmark in LLM under Extreme Scenarios) is a new benchmark designed to test whether Large Language Models (LLMs) can sustain fairness and neutrality when exposed to adversarial prompts constructed to induce bias.,Fairness Benchmark in LLM under Extreme Scenarios,0,0,0
54,Why was FLEX introduced?,"FLEX was introduced because traditional fairness benchmarks, which assume well-intentioned users, may underestimate the inherent safety risks of LLMs to simple adversarial attacks designed to induce bias.","By employing adversarial attacks, we expose hidden biases that may not be surface in traditional benchmarks",0,0,0
55,What are the three main categories of adversarial variants used to construct the FLEX benchmark?,The three categories of adversarial variants are: 1. **Persona Injection** (assigning a negative persona). 2. **Competing Objectives** (assigning conflicting tasks like 'Do Anything Now' or refusal suppression). 3. **Text Attack** (introducing non-noticeable manipulations like typos or paraphrasing).,"Persona Injection, Competing Objectives, Text Attack",0,0,0
56,What key finding was revealed when comparing the accuracy on the source benchmarks ($\text{Acc}_S$) versus the FLEX benchmark ($\text{Acc}_F$)?,$\text{Acc}_F$ (accuracy in extreme scenarios) was consistently and significantly lower than $\text{Acc}_S$ (accuracy in common scenarios). This demonstrates that **being unbiased in common scenarios does not guarantee robustness in extreme situations**.,AccF is consistently lower than that of AccS across different adversarial categories,0,0,0
57,"Which adversarial category was found to be the most effective, especially against open-source models?","The **Competing Objectives** category, which involves simple, direct instructions, was found to be the most effective and straightforward attack, indicating that even the simplest forms of attack often exploit LLM vulnerabilities.",Competing Objectives,0,0,0
58,How do positive shots and negative shots affect the Attack Success Rate (ASR) in the few-shot setting?,"Positive shots (unbiased demonstrations) generally **decreased ASR**, suggesting a positive outcome. Negative shots (biased demonstrations) generally **amplified the threat by increasing ASR**, as models adhered more strongly to the negative instructions.","Negative Shots Amplify the Threat. Models injected with negative samples generally show an increase in ASR, indicating that the adversarial effects are maximized. Particularly, Llama2-13b, which was remarkably robust in the zero-shot setting, exhibits a steep increase in ASR when provided with negative demonstrations.",0,0,0
59,What is the primary concern regarding fairness in Retrieval-Augmented Generation (RAG) systems?,"The primary concern is that RAG systems' multi-component architecture makes it challenging to identify and mitigate biases related to sensitive attributes (like gender and location), as utility-driven optimizations often overlook fairness.","One key challenge in studying fairness in RAG methods comes from the complex, multicomponent architecture they employ",0,0,0
60,What evaluation framework was proposed to assess fairness in RAG methods?,"A systematic fairness evaluation framework tailored for RAG methods was proposed, using a **scenario-based question dataset** focusing on demographic attributes. It evaluates the trade-off between **utility** (Exact Match/ROUGE-1) and **fairness** (Group Disparity/Equalized Odds).",using scenario-based questions and analyzing disparities across demographic attributes,0,0,0
61,What is the trade-off between utility and fairness observed in RAG systems?,"A trade-off exists: models optimized for **utility** (high Exact Match scores) typically **do not show corresponding improvements in fairness**, and vice versa, indicating that focusing on accuracy alone can exacerbate biases.","We evaluate fairness across multiple RAG methods (architectures) using scenario-based questions and benchmarks, revealing the tradeoff between utility and fairness through extensive experiments on real-world datasets.",0,0,0
62,Which RAG component was found to have the most significant influence on both fairness and Exact Match (EM)?,The **Retriever** component was found to have the most significant influence on both fairness and Exact Match (EM).,"As shown in Figure 6, across all RAG methods, EM remains roughly the same between the 8B and 70B models, but bias fluctuates",0,0,0
63,How can increasing the number of retrieved documents impact fairness in RAG?,"For methods like Iter-RetGen and Naive, increasing the number of retrieved documents **significantly improves fairness**. A high initial bias gradually balances out as more documents are retrieved, helping to mitigate gender bias.","We demonstrated that bias can be mitigated by adjusting question formats, increasing the number of retrieved documents, and prioritizing relevant documents from protected groups.",0,0,0
64,What effect did changing the LLM generator size from 8B to 70B parameters have on fairness?,"Increasing the generator size from 8B to 70B parameters caused **bias to fluctuate significantly** (e.g., showing a consistent shift toward a male-favoring bias in the 70B model), while Exact Match remained roughly the same, suggesting a trade-off between model scale and bias direction.","The 70B model shows a consistent shift toward bias favoring males, while the 8B model exhibits more varied results, with both positive and negative biases depending on the method.",0,0,0
65,What simple and effective strategy was identified to mitigate bias by modifying the retrieved documents?,"The most straightforward and effective strategy identified was **adjusting the ranking/priority of relevant documents** in the retrieval results. Manually prioritizing relevant documents from the protected group (e.g., female golden documents) substantially mitigated bias while increasing EM.",affine concept editing (ACE),0,0,0
66,What specific choices in the experimental setup can lead to conflicting bias results when using text completion to measure biases?,"Conflicting bias results can arise from specific choices regarding the **prompt sets**, **metrics**, **automatic tools** used for evaluation, and the **sampling strategies** employed in the experiments.","[1] In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings.",0,0,0
67,What type of language model capability has led researchers to adopt prompting datasets and text completion as a method for quantifying social biases?,The capability of some language models to **generate coherent completions** when given a set of textual prompts has led researchers to pose language generation as a way of identifying and quantifying biases vested in pretrained models.,language models are capable of generating coherent completions given a set of textual prompts,0,0,0
68,What is the main finding regarding measuring social biases in open-ended language generation models?,"The practice of measuring biases through text completion is **prone to yielding contradicting results** under different experiment settings, including choices of prompt sets, metrics, automatic tools, and sampling strategies.","We analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results",0,0,0
69,What is the core argument of the paper regarding the feasibility of developing a generally fair LLM?,"The core argument is that the development of a **generally fair Large Language Model is intractable**, or impossible in a rigorous sense. This is due to inherent challenges in technical fairness frameworks that do not logically extend to the general-purpose AI context.",Counterfactual fairness and other metrics fail to generalize across populations and how realistic counterfactuals would not merely vary in writing style,0,0,0
70,"Why is the ""Fairness Through Unawareness (FTU)"" framework impossible to apply to LLMs by design?","FTU is impossible for LLMs because they are trained on **massive amounts of unstructured data** where **sensitive attributes are pervasive** and cannot be completely excised. LLMs are readily able to infer personal characteristics from this text, making true unawareness unachievable.","FTU would require that an LLM be documentably unaware of the sensitive information, which would require a level of documentation of training data that is unavailable for any state-of-theart LLM today",0,0,0
71,Why can't a general-purpose LLM be made fair across many different contexts?,"Fairness cannot be guaranteed across many contexts because different populations, use cases, and sensitive attributes impose **different and often conflicting fairness requirements**. An attempt to debias for one context may remove or distort essential information for another context, making general cross-context debiasing infeasible.","because of the combinations of populations, use cases, and other factors that impose different fairness requirements",0,0,0
72,"How do LLMs render conventional ""producer-side fairness"" criteria obsolete in multi-sided fairness frameworks?","LLMs render producer-side fairness obsolete because they can **extract information from producers' content without directing users to the original source**, entirely circumventing the producers and their exposure or benefits.","General-purpose LLMs cannot be made fair across many contexts because of the combinations of populations, use cases, and other factors that impose different fairness requirements",0,0,0
73,"Given that general fairness is intractable, what are some promising, tractable future research directions?","Promising directions include: 1. Crafting **standards for developer responsibility** regarding training data transparency. 2. Refining in-depth methods for **context-specific evaluations**. 3. Building **scalable evaluations** that are iterative and participatory (e.g., using AI capabilities to scale up).","Recent approaches focus on model ""self-correction."" There is skepticism that models can currently do this well, but Ganguli et al. show impressive results on bias and discrimination benchmarks ""simply by instructing models to avoid harmful outputs.""",0,0,0
74,What unexpected effect does Knowledge Distillation (KD) have on gender bias in smaller models compared to their larger source models?,"After knowledge distillation, the smaller model is found to be **more biased by gender** compared to the source large model, meaning KD amplifies gender bias.","[1] However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model. This paper studies what causes gender bias to increase after the knowledge distillation process.",0,0,0
75,What two factors in the Knowledge Distillation process were identified as contributing to gender bias amplification?,The two contributing factors are: 1. The **limited capacity of the student model** (smaller models show more severe bias). 2. The **cross-entropy loss term** ($\boldsymbol{L_{ce}}$) between the logit distribution of the student and teacher models.,cross-entropy loss between the logits and the model capacity,0,0,0
76,"What mitigation technique was proposed to reduce gender bias amplification during knowledge distillation, and where was it most effective?",The technique proposed is applying **mixup** to the knowledge distillation process. It was most effective when applied to the **student's input embedding** and the **teacher's output logit** for gender-related words.,"[2] However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model. Moreover, we suggest applying a variant of the mixup on knowledge distillation, which is used to increase generalizability during the distillation process, not for augmentation. By doing so, we can significantly reduce the gender bias amplification after knowledge distillation.",0,0,0
77,"What is the proposed novel, automated mechanism for debiasing LLMs?",The proposed mechanism is **automated dataset augmentation** based on the concept of **bias producers** (broad bias creators like ethnicity or sexuality) and **biasers** (specific examples).,Dataset augmentation,0,0,0
78,What kind of industries is the automated debiasing mechanism most effective for?,"This approach is most effective for **""restricted industries""** (e.g., defense, medical, financial) where data is limited due to confidentiality or availability.","[2] More specifically, we propose a novel debiasing mechanism aimed at LLMs in various industries but most effective for ""restricted industries"" through automated dataset augmentation",0,0,0
79,How does the automated dataset augmentation technique work to mitigate bias?,"The technique involves: 1. Identifying a bias producer/biasers. 2. Iteratively **substituting the biaser** in an entry with other biaser members to create new entries. 3. Applying **content morphism** to each new entry (creating augmented/summarized versions). This is an **autonomous** process, eliminating human annotator bias.","Such an augmentation algorithm is autonomous and, therefore, reduces the implicit bias introduced by annotators and other human means of defining bias",0,0,0
80,What new metrics were proposed to quantify bias in datasets and models?,Two new metrics were proposed: the **db-index** (dataset bias index) for quantifying bias in datasets and the **mb-index** (model bias index) for quantifying bias in LLM performance.,db-index and mb-index,0,0,0
81,What specific formula and components define the **mb-index** (model bias index)?,"The mb-index is a normalized metric that quantifies bias in an LLM's performance per data entry trained on, defined by the formula: $\text{mb-index} = \frac{\text{Perplexity} \times \text{Stereotype Score}}{\text{Dataset Size}}$. It combines performance (Perplexity) and bias (Stereotype Score) relative to the training data size.","[2] We also define two new metrics for quantifying bias about both datasets and models, the db-index and mb-index, respectively. These metrics provide Computers 2024, 13, 141.",0,0,0
82,What primary limitation of simple anti-bias prompts was demonstrated in realistic high-stakes scenarios like hiring?,"Simple anti-bias prompts, effective in controlled settings, were shown to be **fragile and unreliable** when realistic contextual details (like company culture and selective constraints) were introduced, leading to significant race and gender biases (up to 12% differences in interview rates).","We test four different anti-bias prompts of increasing specificity and length: a basic legal reminder about discrimination laws, a comprehensive warning from Tamkin et al. (2023) emphasizing legal ramifications, instructions to check for hidden assumptions, and a detailed equity framework outlining fairness principles in hiring processes.",0,0,0
83,What was the consistent direction of the bias observed when realistic context was introduced?,The bias consistently favored **Black candidates over White candidates** and **female candidates over male candidates** across all tested models and contexts where realistic context induced bias.,favored Black candidates over White candidates and female candidates over male candidates,0,0,0
84,Did Chain-of-Thought (CoT) monitoring successfully detect the bias observed when realistic context was introduced?,"No. Chain-of-Thought (CoT) monitoring **failed to detect this bias**, as models consistently rationalized biased outcomes with neutral-sounding justifications despite demonstrably biased decisions.","[1] This pattern suggests that practitioners cannot reliably predict or prevent bias through prompt engineering alone, as subtle changes in evaluation format or task constraints can dramatically alter model behavior. 7 Preprint Chain of thought monitoring does not detect biased behavior. Recent work has proposed monitoring chain-of-thought (CoT) explanations for detecting undesired model behaviors (Baker et al., 2025). We investigated the efficacy of this approach in our experiments",0,0,0
85,What internal bias mitigation technique did the study propose?,"The study proposed an internal bias mitigation strategy using **Affine Concept Editing (ACE)**, which identifies and neutralizes sensitive attribute directions (race and gender) within model activations at inference time.",affine concept editing,0,0,0
86,How robust was the proposed internal bias mitigation technique?,"The technique proved **robust**, consistently reducing measurable bias to very low levels (typically under 1%, always below 2.5%) across all tested models and challenging contextualized settings, while largely maintaining model performance.","[1] Our intervention effectively mitigates bias in this scenario. Furthermore, the impact on general model capabilities, as measured by MMLU, is minimal",0,0,0
87,"How did LLMs demonstrate the ability to infer demographics, bypassing standard anonymization techniques?","LLMs demonstrated the ability to infer demographics and become biased from **subtle cues** beyond explicit names or pronouns, such as from **college affiliations** (e.g., HBCUs like Morehouse College).","From subtle cues beyond explicitly stated names or pronouns, such as from writing style or linguistic patterns (Chen et al., 2024; Staab et al., 2024).",0,0,0
88,What is StereoSet?,"StereoSet is a **large-scale natural dataset in English** containing **16,995 Context Association Tests (CATs)** (triplets) to measure stereotypical biases in pretrained language models.","a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion",0,0,0
89,What is the primary purpose of StereoSet?,"The primary purpose of StereoSet is to measure stereotypical biases in pretrained language models with respect to **gender, profession, race, and religion**.",measure the biases of pretrained language models,0,0,0
90,"What two types of Context Association Tests (CATs) are used in StereoSet, and how are they structured?","The two types of CATs are: 1. **Intrasentence CATs**, which measure bias at the sentence level using a fill-in-the-blank style context. 2. **Intersentence CATs**, which measure bias at the discourse level, providing a context sentence followed by three associative sentences (stereotype, anti-stereotype, unrelated).",Intrasentence and intersentence,0,0,0
91,What does the Idealized CAT (ICAT) score measure?,The ICAT score combines the Language Modeling Score (lms) and the Stereotype Score (ss) into a single metric to measure how close a model is to an idealistic language model.,"We combine both lms and ss into a single metric called the idealized CAT (icat) score based on the following axioms: 1. An ideal model must have an icat score of 100, i.e., when its lms is 100 and ss is 50, its icat score is 100. 2. A fully biased model must have an icat score of 0, i.e., when its ss is either 100 (always prefer a stereotype over an anti-stereotype) or 0 (always prefer an anti-stereotype over a stereotype), its icat score is 0. 3. A random model must have an icat score of 50, i.e., when its lms is 50 and ss is 50, its icat score must be 50.",0,0,0
92,What value represents an ideal language model under the ICAT score?,"An **ideal language model** has an **ICAT score of 100**, achieved when its Language Modeling Score (lms) is 100 (always prefers meaningful contexts) and its Stereotype Score (ss) is 50 (shows no stereotypical preference).","[2] An interpretation of icat is that it represents the language modeling ability of a model to behave in an unbiased manner while excelling at language modeling. 6.3 Baselines IDEALLM We define this model as the one that always picks correct associations for a given target term context. It also picks equal number of stereotypical and anti-stereotypical associations over all the target terms. So the resulting lms, ss and icat scores are 100, 50 and 100 respectively.",0,0,0
93,Which family of models exhibited the most idealistic behavior (highest ICAT score) on StereoSet?,"The **GPT2 family of models** exhibited relatively more idealistic behavior and a good balance between the language modeling and stereotype scores compared to models like BERT, RoBERTa, and XLNet.",ROBERTA and XLNET,0,0,0
94,What is the technique of zero-shot self-debiasing?,"Zero-shot self-debiasing is a scalable, prompting-based technique that uses nothing other than the LLM itself to elicit recognition and avoidance of stereotypes with a simple prompt.","In short, with simple prompts that generalize across various social groups, zero-shot selfdebiasing can significantly and consistently reduce stereotyping",0,0,0
95,What primary drawback of traditional bias mitigation techniques does zero-shot self-debiasing solve?,"Zero-shot self-debiasing solves the drawback that most traditional techniques require computationally expensive modifications to the training data, model parameters, fine-tuning, or decoding strategy, which may be infeasible without access to a trainable model.","scalability, computational feasibility, or generalizability to different dimensions of bias",0,0,0
96,What were the two approaches proposed for zero-shot self-debiasing?,"The two approaches introduced were: 1. **Self-debiasing via Explanation**, which asks the model to first explain the invalid assumptions in the answer choices before answering. 2. **Self-debiasing via Reprompting**, which asks the LLM to answer a second time with the instruction to ""Remove bias from your answer"".",Bias Benchmark for QA (BBQ),0,0,0
97,How did the two self-debiasing techniques compare in reducing stereotypes across the social groups tested?,Both techniques significantly and consistently reduced stereotyping across the nine social groups tested. The **Reprompting approach** showed **greater reductions in bias** (overall score decreased from 0.136 to 0.023) and was more token-efficient. The **Explanation approach** correctly identified the stereotypical assumptions in the answers and reduced the overall bias score to 0.045.,"[2] At a high level, we find that, regardless of the varying baseline levels of bias the LLM exhibits for each social group, both self-debiasing techniques substantially reduce the degree of stereotyping.",0,0,0
98,What key observation was made about GPT-3.5's responses to occupation-related queries based on gender?,"An invalid inference was observed where the LLM provided conceptually different answers for male and female data engineers, despite the ambiguous query. For a male data engineer, the recommendations were technical, but for a female data engineer, they included non-technical areas (e.g., ""Soft skills""), reflecting stereotypical assumptions.","invalid inferences for occupation-related queries, which can result in conceptually different answers for males and females",0,0,0
99,What is prompt brittleness in Large Language Models?,"Prompt brittleness is the high sensitivity of In-Context Learning (ICL) performance to variations in design settings, such as example selection, order, and prompt formatting.",it is demonstrated in the literature that this instability of prompt arises from LLMs' inherent bias towards predicting certain answers,0,0,0
100,What is the internal cause of prompt brittleness in Large Language Models?,"The internal cause of prompt brittleness is the **inherent bias** in LLMs towards predicting certain answers, which originates from the internal mechanisms of the model.","LLMs' inherent bias towards predicting certain answers [Zhao et al., 2021]",0,0,0
101,What specific internal components of an LLM were investigated as causes of bias?,The specific internal components investigated were **Feedforward Neural Networks (FFNs) vectors** and **attention heads**.,"The data used to train an LLM may be drawn from a non-representative sample of the population, which can cause the model to fail to generalize well to some social groups.",0,0,0
102,What biases were linked to the investigated internal components of an LLM?,"Biases linked to the internal components include **vanilla label bias** (linked to FFN vectors), and **recency bias** and **selection bias** (linked to attention heads).",More recent work finds that modern LLMs exhibit implicit biases in word associations and simulated decision-making scenarios,0,0,0
103,What is UniBias?,UniBias is an **inference-only method** designed to **unveil and eliminate biased components** by directly manipulating the LLM's internal structure.,an inference-only method designed to identify and eliminate biased FFN vectors and attention heads in LLMs,0,0,0
104,How does UniBias mitigate bias internally?,"UniBias mitigates bias internally by identifying biased FFN vectors and attention heads based on three criteria (Relatedness, Bias, and Low Variance) and **masking (eliminating)** these biased components during inference.",By addressing their root causes internally from LLMs,0,0,0
105,What were the key experimental results regarding the performance of UniBias?,"UniBias significantly **enhanced ICL performance** and successfully **alleviated prompt brittleness**, achieving stable accuracy with variations consistently less than 4% under various prompt perturbations.","Extensive experimental results demonstrate that LLMs, from which biased components have been removed, consistently outperform their original counterparts by a significant margin",0,0,0
106,What are the three primary sources contributing to bias in Large Language Models?,The three primary sources are: 1. **Training data bias**. 2. **Embedding bias**. 3. **Label bias** (from human annotators or RLHF).,"i) Training data bias. The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.",0,0,0
107,How are metrics for quantifying bias in LLMs categorized in the proposed taxonomy?,Metrics are categorized based on the data format they utilize: 1. **Embedding-based Metrics**. 2. **Probability-based Metrics**. 3. **Generation-based Metrics**.,"i) embedding-based metrics, ii) probability-based metrics, and iii) generation-based metrics",0,0,0
108,What are the four stages used to categorize algorithms for mitigating bias in LLMs?,Mitigation techniques are structured according to their intervention point in the LLM workflow: 1. **Pre-processing**. 2. **In-training**. 3. **Intra-processing**. 4. **Post-processing**.,"pre-processing, in-training, intra-processing, and postprocessing",0,0,0
109,What is the Challenge regarding Counterfactual Data Augmentation (CDA)?,"The key challenge is **inconsistent data quality**, potentially leading to the generation of unnatural or irrational sentences (e.g., illogical replacement of height/weight associated with one gender to another).",Many existing CDA approaches rely on word substitution techniques using manually compiled word-pair dictionaries.,0,0,0
110,What is a potential future research direction to address the Challenge regarding Counterfactual Data Augmentation (CDA)?,A potential future research direction is to explore **more rational replacement strategies** or integrate techniques to filter or optimize the generated data to prevent inconsistency and irrational output.,CDA as a bias mitigation technique adopt a word substitution approach based on dictionaries.,0,0,0
111,What are the key motivations for the emerging field of LLMs Unlearning?,"The key motivations for LLMs unlearning include compliance with **data privacy regulations** and copyright protection, as well as enabling applications like model detoxification (reducing harmful/toxic content), model correction, and jailbreaking defence.","For machine unlearning, the initial goal is to remove unwanted data from the training data, namely private data as requested, without retraining the model from scratch or compromising the model perfor mance on retaining data",0,0,0
112,What new objective is proposed for LLMs unlearning?,"A novel fourth objective, **Robustness**, is proposed, alongside the established goals of Effectiveness, Efficiency, and Utility.","To remove unwanted data from the training data, namely private data as requested, without retraining the model from scratch or compromising the model perfor mance on retaining data",0,0,0
113,Why is the new objective of Robustness important for LLMs unlearning?,"Robustness is crucial because it ensures the unlearned model can **maintain its intended functionality** in the presence of adversarial inputs, jailbreaking attempts, or other exploitation techniques, thereby preventing unlearned data from resurging.","According to the Institute of Electrical and Electronics Engineers (IEEE) glossary, robustness is the degree to which a system can perform accurately under invalid inputs or stressful conditions",0,0,0
114,What are the three main categories of LLMs unlearning algorithms?,The three main categories of LLMs unlearning algorithms are: 1. **Parameter Modification**. 2. **Input Modification**. 3. **Robust Unlearning**.,"parameter modification, input modification, and robust unlearning",0,0,0
115,What are the major limitations of Input Modification methods (like In-context Unlearning) for achieving fairness?,"Input modification methods are considered unreliable because they offer no formal unlearning guarantee and are vulnerable to adversarial attacks. Crucially, they require storing unwanted data for prompting construction, which contradicts the goal of data privacy established by regulations like GDPR.","these approaches yield no forgetting guarantee and offer no data privacy in the context of ""the right to be forgotten"" due to its requirement of unwanted data storage",0,0,0
116,Which two threat models are used for robustness evaluations to test if unlearned data can be recovered?,"1. **Membership Inference Attacks (MIA)**, used to determine if a data point was in the training set. 2. **Dynamic Unlearning Attack (DUA)**, a type of adversarial suffix optimisation, and other relearning attacks, used to recover unlearned data.",adversarial attacks and mali cious interrogations,0,0,0
117,What three main taxonomies are used to unify the literature on bias and fairness in LLMs?,The three main taxonomies are: 1. **Metrics for Bias Evaluation**. 2. **Datasets for Bias Evaluation**. 3. **Techniques for Bias Mitigation**.,"metrics and datasets, and one for mitigation",0,0,0
118,How is Social Bias broadly defined in the context of LLMs?,Social bias is broadly defined as **disparate treatment or outcomes between social groups** that arise from historical and structural power asymmetries.,"Unless otherwise specified, our use of ""bias"" refers to social bias, defined in Definition 7.",0,0,0
119,What are the two main categories of harm encompassed by Social Bias?,"The two main categories of harm are **Representational Harms** (e.g., stereotyping, misrepresentation) and **Allocational Harms** (e.g., direct and indirect discrimination).","misrepresentation, stereotyping, disparate system performance, derogatory language, and exclusionary norms) and allocational harms (direct discrimination and indirect discrimination)",0,0,0
120,What are three core Fairness Desiderata proposed for LLMs to generalize fairness notions beyond simple classification tasks?,Three core desiderata include: 1. **Invariance** (Definition 9): Output must be identical when the social group in the input is substituted. 2. **Equal Social Group Associations** (Definition 10): A neutral word must be equally likely regardless of the social group mentioned. 3. **Fairness Through Unawareness** (Definition 8): Output must be independent of an explicitly used social group.,"We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation",0,0,0
121,"What are the major limitations of Pre-Processing Mitigation techniques, particularly those relying on Data Augmentation?","Pre-processing techniques, especially data augmentation, are limited by **unscalable and incomplete word lists**, can introduce **factuality or grammatical errors** when swapping terms, and rely on the **tenuous assumption** that social groups are interchangeable, which ignores the underlying complexities of oppression.","Data augmentation techniques swap terms using word lists, which can be unscalable and introduce factuality errors (Kumar et al. 2023b). Furthermore, word lists are often limited in length and scope, may depend on proxies (e.g., names as a proxy for gender) that are often tied to other social identities, and utilize word pairs that are not semantically or connotatively equivalent (Devinney, Bj orklund, and Bj orklund 2022).",0,0,0
122,What is the primary concern regarding fairness in Retrieval-Augmented Generation (RAG) systems?,"The primary concern is that RAG systems' multi-component architecture makes it challenging to identify and mitigate biases related to sensitive attributes (like gender and location), as utility-driven optimizations often overlook fairness.","One key challenge in studying fairness in RAG methods comes from the complex, multicomponent architecture they employ",0,0,0
123,What is FLEX?,FLEX (Fairness Benchmark in LLM under Extreme Scenarios) is a new benchmark designed to test whether Large Language Models (LLMs) can sustain fairness and neutrality when exposed to adversarial prompts constructed to induce bias.,Fairness Benchmark in LLM under Extreme Scenarios,0,0,0
