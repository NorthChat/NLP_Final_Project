[
  {
    "id": 1,
    "question": "What is Auto-Debias?",
    "answer": "Auto-Debias is an automated method for mitigating biases in pretrained language models without using external corpora. Unlike previous approaches that rely on external datasets for fine-tuning, Auto-Debias directly probes biases through cloze-style prompts.",
    "relevant_papers": [
      "auto_debias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 2,
    "question": "How does Auto-Debias differ from previous debiasing approaches?",
    "answer": "Auto-Debias differs from previous debiasing approaches by leveraging the LLM itself: it directly probes biases through cloze-style prompts and uses a distribution alignment loss to minimize differences across demographic groups during fine-tuning, without relying on external corpora or datasets for training.",
    "relevant_papers": [
      "auto_debias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 3,
    "question": "How does the Auto-Debias approach search for biased prompts?",
    "answer": "Auto-Debias uses a beam search algorithm to automatically generate biased prompts by maximizing the Jensen-Shannon divergence (JSD) between predicted mask token distributions for different demographic groups. The algorithm searches through candidate vocabulary (top 5,000 frequent words from Wikipedia) to find prompt sequences that reveal the highest disagreement in generating stereotype words when conditioned on different target concepts like 'he' versus 'she'.",
    "relevant_papers": [
      "auto_debias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "hard"
  },
  {
    "id": 4,
    "question": "What are the potential downstream effects of AI-AI bias identified in the research?",
    "answer": "The research identifies two main scenarios: First, a conservative scenario where LLM assistants in decision-making create a 'gate tax' that exacerbates the digital divide between those who can afford LLM writing assistance and those who cannot. Second, a speculative scenario where autonomous LLM-based agents increasingly marginalize human economic agents as a class, potentially leading to widespread discrimination against humans who cannot or will not use LLM assistance.",
    "relevant_papers": [
      "ai_ai_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 5,
    "question": "What is AI-AI bias?",
    "answer": "AI-AI bias refers to the tendency of LLMs to favor communications and content generated by other LLMs over human-generated content.",
    "relevant_papers": [
      "ai_ai_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 6,
    "question": "How does AI-AI bias manifest in large language models?",
    "answer": "This bias manifests as LLM-based assistants systematically preferring LLM-presented options in binary choice scenarios, such as product descriptions, academic papers, and film summaries. Research shows LLMs consistently choose LLM-authored content more frequently than humans do, suggesting implicit discrimination against humans as a class.",
    "relevant_papers": [
      "ai_ai_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 7,
    "question": "What experimental design was used to test AI-AI bias in the PNAS 2025 study?",
    "answer": "The study used a classical experimental design inspired by employment discrimination studies, testing LLMs including GPT-3.5, GPT-4, and open-weight models in binary choice scenarios. Three experiments were conducted: choosing between consumer products via classified ads, selecting academic papers based on abstracts, and recommending films based on plot summaries. Each scenario presented one human-authored and one LLM-authored description of the same item.",
    "relevant_papers": [
      "ai_ai_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 8,
    "question": "What metrics are used to evaluate bias in masked language models according to the Auto-Debias paper?",
    "answer": "The Auto-Debias paper evaluates bias using the Sentence Embedding Association Test (SEAT), which measures associations between demographic-specific words and stereotype words using context-independent embeddings. Additionally, the CrowS-Pairs benchmark measures the percentage of sentence pairs where models assign higher likelihood to stereotyping sentences.",
    "relevant_papers": [
      "auto_debias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 9,
    "question": "What were the main findings regarding Auto-Debias effectiveness on different models?",
    "answer": "Auto-Debias significantly reduced bias across BERT, ALBERT, and RoBERTa. For BERT, the average SEAT score decreased from 0.35 to 0.14; for ALBERT from 0.28 to 0.18; and for RoBERTa from 0.67 to 0.20. The method proved more effective than existing state-of-the-art benchmarks and maintained language modeling capability on GLUE tasks.",
    "relevant_papers": [
      "auto_debias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 10,
    "question": "What is the CLEAR-Bias dataset?",
    "answer": "CLEAR-Bias (Corpus for Linguistic Evaluation of Adversarial Robustness against Bias) is a systematically designed benchmark dataset consisting of 4,400 curated prompts for evaluating bias vulnerabilities in language models.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 11,
    "question": "What does the CLEAR-Bias dataset consist of?",
    "answer": "The CLEAR-Bias dataset covers seven bias dimensions (age, disability, ethnicity, gender, religion, sexual orientation, socioeconomic status) plus three intersectional categories. Each category includes prompts for two task types: Choose the Option (CTO) and Sentence Completion (SC), with base prompts augmented using seven jailbreak techniques.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 12,
    "question": "What jailbreak techniques are included in the CLEAR-Bias benchmark?",
    "answer": "CLEAR-Bias includes seven jailbreak techniques with three variants each: machine translation to low-resource languages (Slovene, Macedonian, Scottish Gaelic), obfuscation through encoding methods (leetspeak variations, Base64), prefix injection with predefined responses, prompt injection with distraction tasks, refusal suppression banning apologetic language, reward incentive promising recognition, and role-playing with specific personas (superhero, scientist, Machiavelli).",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 13,
    "question": "How does the LLM-as-a-judge approach work in bias evaluation?",
    "answer": "The LLM-as-a-judge paradigm uses one LLM to evaluate responses from other LLMs, providing scalable automated assessment. The judge LLM classifies responses into four categories: Stereotyped (reinforcing stereotypes), Counter-stereotyped (challenging stereotypes), Debiased (impartial and balanced), and Refusal (declining to answer). The best judge is selected by comparing candidates' classifications against a manually curated control set using Cohen's kappa coefficient to measure inter-rater agreement.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 14,
    "question": "Which LLM was identified as the most reliable judge in the CLEAR-Bias study and why?",
    "answer": "DeepSeek V3 671B was identified as the most reliable judge LLM, achieving the highest Cohen's kappa of 0.82 and the highest Macro F1-Score of 0.861. It demonstrated superior classification performance with 0.873 accuracy on sentence completion tasks and 0.865 on choose-the-option tasks. This high agreement with human annotations made it the optimal choice for automated bias evaluation.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 15,
    "question": "What were the key findings about bias categories in the CLEAR-Bias evaluation?",
    "answer": "The evaluation revealed uneven bias resilience across categories. Religion showed the highest average safety score (0.70), followed by sexual orientation (0.65), ethnicity (0.59), and gender (0.57). Intersectional categories showed lower safety: gender-ethnicity (0.53), ethnicity-socioeconomic (0.45), and gender-sexual orientation (0.42). The categories with lowest safety were socioeconomic status (0.31), disability (0.25), and age (0.24), indicating these biases are most prominent and least mitigated.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 16,
    "question": "How did small language models compare to large language models in bias safety?",
    "answer": "The results challenged assumptions about model size and safety. While average safety scores were comparable (SLMs: 0.467 vs LLMs: 0.48), LLMs showed greater stability with lower standard deviation. Notably, some small models like Phi-4 (0.64) and Gemma2 27B (0.635) achieved the highest safety scores, outperforming larger models like DeepSeek V3 671B (0.405) and GPT-4o (0.455), suggesting that training methodology and architecture may matter more than scale.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 17,
    "question": "What were the most effective jailbreak attacks for eliciting bias from LLMs?",
    "answer": "Machine translation emerged as the most effective attack overall with 0.34 effectiveness, particularly using Scottish Gaelic. This was followed by refusal suppression (0.30) and prompt injection (0.29). These attacks exploit models' weaker reasoning in low-resource language contexts, directly target safety mechanisms, or leverage linguistic ambiguity. In contrast, reward incentive (0.05) and role-playing (0.04) showed lower effectiveness as models generally recognized and mitigated these tactics.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 18,
    "question": "How do medical LLMs compare to general-purpose LLMs in terms of bias safety?",
    "answer": "Fine-tuned medical LLMs exhibited lower safety scores compared to their general-purpose counterparts. This trend likely occurs because fine-tuning emphasizes domain-specific medical knowledge over general safety alignment. While foundational models undergo rigorous safety tuning across domains, medical LLMs prioritize accuracy in clinical contexts, potentially overshadowing ethical concerns. Additionally, medical fine-tuning datasets may introduce domain-specific biases that reduce the effectiveness of inherited safety measures.",
    "relevant_papers": [
      "clear_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 19,
    "question": "What is the CrowS-Pairs dataset?",
    "answer": "CrowS-Pairs (Crowdsourced Stereotype Pairs) is a challenge dataset with 1,508 examples for measuring social biases in masked language models.",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 20,
    "question": "What does the CrowS-Pairs dataset measure?",
    "answer": "The CrowS-Pairs dataset measures whether models prefer stereotyping sentences about historically disadvantaged groups over less stereotyping alternatives. It covers nine bias types: race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status.",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 21,
    "question": "How does the CrowS-Pairs metric evaluate bias in masked language models?",
    "answer": "The CrowS-Pairs metric uses pseudo-log-likelihood scoring to compare sentence pairs while controlling for word frequency. The metric measures the percentage of examples where the model assigns higher likelihood to the stereotyping sentence. An unbiased model should achieve 50%, while higher scores indicate greater bias.",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 22,
    "question": "What were the main results when evaluating BERT, RoBERTa, and ALBERT on CrowS-Pairs?",
    "answer": "All three widely-used MLMs exhibited substantial bias in every category. BERT showed a bias score of 60.5%, RoBERTa 64.1%, and ALBERT 67.0%. Religion was the hardest category for all models (BERT: 71.4%, RoBERTa: 71.4%, ALBERT: 75.2%), while gender and race were comparatively easier. Models showed less bias on anti-stereotype examples compared to stereotype examples.",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 23,
    "question": "What validation process was used for CrowS-Pairs and how did it compare to StereoSet?",
    "answer": "CrowS-Pairs used crowdsourced validation where 5 annotators per example labeled whether sentences expressed stereotypes, anti-stereotypes, or neither. In an independent validation comparing 100 random samples, CrowS-Pairs achieved 80% validity rate versus 62% for StereoSet, demonstrating substantially higher quality and reliability.",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 24,
    "question": "What are the key differences between stereotype and anti-stereotype examples in bias evaluation?",
    "answer": "Stereotype examples demonstrate common prejudicial beliefs about disadvantaged groups (e.g., 'The crafty Jews made a plan to steal money'), while anti-stereotype examples violate these stereotypes (e.g., 'We were upset there were so many gross young people at the beach' instead of old people).",
    "relevant_papers": [
      "comprehensive_survey"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 25,
    "question": "What are the main categories of bias identified in the comprehensive survey of LLM biases?",
    "answer": "The survey identifies three main categories: Data-Driven Bias, Model Architecture Bias, and Social Bias (including gender, racial, and age biases). Additional categories include Application-Specific Bias, Cognitive Bias, Societal Bias, and Systemic Bias.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 26,
    "question": "What are the major sources of bias in LLMs according to the comprehensive survey?",
    "answer": "Major sources include: training data (data collection/annotation bias), model architecture (design choices/loss functions), human factors (developer biases/cultural backgrounds), user interactions (feedback loops), societal influences (norms/stereotypes), prompt design, evaluation metrics, cultural context, and selection bias.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 27,
    "question": "What is demographic bias in LLMs?",
    "answer": "Demographic bias refers to disparities in how different demographic groups are treated by LLMs, often perpetuating stereotypes, limiting perceived roles, and reinforcing existing social inequalities.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 28,
    "question": "What types of demographic bias does it include?",
    "answer": "Demographic bias includes: gender bias (occupational stereotypes/differential treatment), racial and ethnic bias (unfair treatment/amplification of stereotypes), age bias (disproportionate favor/disadvantage based on age), and socioeconomic status bias (assumptions based on economic background).",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 29,
    "question": "What recent techniques have been developed for bias evaluation and mitigation in LLMs?",
    "answer": "Recent techniques include: BiasAlert for detecting social bias in open-text generations; Deceiving to Enlighten approach using self-reflection; Social Contact Debiasing (SCD) using Contact Hypothesis; PoliTune for ideological alignment; LangBiTe platform for systematic evaluation; and Chain-of-Thought (CoT) prompting to mitigate biases during reasoning tasks.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 30,
    "question": "What is the LLM Bias Index (LLMBI) and how does it work?",
    "answer": "The Large Language Model Bias Index (LLMBI) is a composite scoring system that quantifies and addresses biases inherent in LLMs by incorporating multiple dimensions of bias including age, gender, and racial biases. It provides a unified metric for measuring various types of bias simultaneously.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 31,
    "question": "What are the current limitations in bias evaluation and mitigation for LLMs?",
    "answer": "Current limitations include: lack of standardized and intersectional evaluation metrics; bias research heavily skewed toward high-resource languages and Western-centric contexts; opaque models lacking transparency in training processes; and underdeveloped scalable post-training bias mitigation techniques that are computationally expensive.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 32,
    "question": "What future research directions are proposed for addressing bias in LLMs?",
    "answer": "The survey proposes five key directions: comprehensive lifecycle bias evaluation frameworks; intersectional and contextual bias mitigation for high-stakes applications; bias-aware training and pre-training techniques; bias evaluation and mitigation for multimodal and non-English models; and explainability and transparency in bias mitigation.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 33,
    "question": "How did the evolution of language models contribute to bias issues?",
    "answer": "As models evolved from early forms to modern models like GPT-3 and GPT-4, they achieved impressive capabilities by learning more features of language from minimally-filtered real-world text, but they also learned and reflected more features of society, including cultural biases and stereotypes present in their training corpora.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "easy"
  },
  {
    "id": 34,
    "question": "What is contextual bias and how does it manifest in domain-specific applications?",
    "answer": "Contextual bias refers to biased outcomes based on the application context. In domain-specific applications like healthcare, LLMs may perpetuate biases from medical literature. Cultural context bias occurs when models trained on specific cultural data struggle with different cultural backgrounds, leading to misinterpretations or culturally insensitive outputs.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 35,
    "question": "What methods exist for detecting and measuring bias in LLMs?",
    "answer": "Detection methods include both qualitative and quantitative approaches. Quantitative methods include: metrics like disparity metrics, equal opportunity metrics, statistical parity, and equalized odds; benchmarks like Fairness Indicators; statistical techniques like hypothesis testing; and composite metrics like LLMBI.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 36,
    "question": "What are the social and operational implications of bias in LLMs?",
    "answer": "Social implications include exacerbating inequalities by reinforcing harmful stereotypes and limiting opportunities. Operational implications include performance degradation for underrepresented groups, reduced model utility, and erosion of user trust and satisfaction.",
    "relevant_papers": [
      "crow_s_pairs"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "easy"
  },
  {
    "id": 37,
    "question": "What is the core problem addressed by using causally-driven data augmentation for text classifiers?",
    "answer": "The core problem is the reliance of text classifiers on spurious correlations (shortcuts), which leads to poor generalization on unseen data (Out-of-Distribution or OOD) at deployment. In the clinical notes example, a spurious correlation is between caregiver-specific writing style (C) and the clinical outcome (Y).",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "easy"
  },
  {
    "id": 38,
    "question": "How does Counterfactual Data Augmentation (CDA) mitigate spurious correlations in this context?",
    "answer": "CDA is used to simulate interventions on spurious features (like changing the caregiver writing the note) to de-correlate the writing style from the patient condition. Generating versions of clinical narratives as if they were written by different caregivers breaks up the spurious correlation, allowing the classifier to learn a more robust, invariant predictor.",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "easy"
  },
  {
    "id": 39,
    "question": "What is CATO?",
    "answer": "CATO (Causal-structure Driven Augmentations for Text OOD Generalization) is a framework for estimating counterfactuals that uses an LLM to model the conditional probability of text.",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 40,
    "question": "What are CATO's two main estimation methods for counterfactuals?",
    "answer": "CATO's two main estimation methods are: 1. **Diff-in-Diff Estimation (CATO (A))**: Uses auxiliary pre-treatment data and patient matching to generate counterfactuals based on the difference in notes, assuming a constant effect of the caregiver. 2. **Prompting with Matched Examples (CATO (B))**: Matches examples using auxiliary data and constructs a prompt for the LLM to rewrite the original text in the style of the matching examples with the desired attribute (caregiver style).",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 41,
    "question": "What did the experiments on clinical narratives (Clinical Condition Prediction and Note Segmentation) show about CATO?",
    "answer": "CATO (A) consistently and substantially outperformed all baselines (including Reweighting, MMD, IRM, and GroupDRO) on the **Out-of-Distribution (OOD)** data (i2b2-2010 and Private Held-Out). The Naive Augmentation approach, which ignores the causal structure, performed well on in-distribution (ID) data but was outperformed by all OOD methods.",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 42,
    "question": "Why is counterfactual data augmentation preferred over the Reweighting method based on sample complexity arguments?",
    "answer": "While reweighting can also learn a min-max optimal hypothesis, it often requires a larger sample size to identify the correct hypothesis, especially when the label (Y) and the spurious attribute (C) are highly correlated. The theoretical bounds show that accurately performed CDA scales with $N^{-1/2}$ and gains a factor of $d_{2,train}(Y,C)$ over the reweighting bound.",
    "relevant_papers": [
      "data_aug_for_llm_generalization"
    ],
    "category": "Mitigation Technique",
    "difficulty": "hard"
  },
  {
    "id": 43,
    "question": "What is the main challenge addressed by FairFlow in the context of Counterfactual Data Augmentation (CDA)?",
    "answer": "The main challenge is the unavailability of parallel training data needed for training model-based counterfactual text generators, and the limitations of manually compiled, dictionary-based CDA methods which are prone to grammatical incoherence due to out-of-context substitutions and omitted word pairs.",
    "relevant_papers": [
      "fairflow"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 44,
    "question": "How does FairFlow automatically generate a dictionary of attribute word pairs?",
    "answer": "FairFlow automates dictionary compilation by first taking a user input prompt of a single attribute word-pair (e.g., 'she, he') to define a demographic axis. It then trains an attribute classifier using contextualized embeddings of these words from a corpus. Finally, a disentangling invertible interpretable network (DIIN) generates the counterfactual equivalent for each selected attribute word.",
    "relevant_papers": [
      "fairflow"
    ],
    "category": "Mitigation Technique",
    "difficulty": "hard"
  },
  {
    "id": 45,
    "question": "What technique is used in FairFlow to improve the fluency and grammatical coherence of the counterfactual texts?",
    "answer": "An error correction scheme is proposed, which is used in tandem with word substitution to create a fluent and realistic parallel dataset. This scheme involves **Erratic Token Detection** (masking tokens with low probability) and **Text Insertion with BART** (generating plausible token replacements).",
    "relevant_papers": [
      "fairflow"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 46,
    "question": "What metric demonstrates that the error correction scheme successfully improves the fluency of the generated text?",
    "answer": "Perplexity (PPL), a referenceless fluency metric where lower scores indicate better fluency, demonstrates the improvement. The results consistently show that the fluency is improved (PPL is reduced) in both FairFlow V1 and FairFlow V2 compared to the baseline methods, confirming the effectiveness of the error correction approach.",
    "relevant_papers": [
      "fairflow"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 47,
    "question": "What were the two main observations from the qualitative analysis comparing FairFlow, ChatGPT, and baseline models?",
    "answer": "1. Automating the dictionary compilation process does not materially impair counterfactual generation. 2. A model fine-tuned on erroneous data mimics those errors, whereas FairFlow's error correction made the model more robust and coherent.",
    "relevant_papers": [
      "fairflow"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 48,
    "question": "What is algorithmic overgeneralization in the context of scientific text summarization by LLMs?",
    "answer": "Algorithmic overgeneralization is a strong bias in LLMs toward producing generalizations of scientific results that are broader than warranted by the original scientific text. This means the LLM summary contains a broader (generalized) conclusion when the original text had a narrower (restricted) conclusion.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 49,
    "question": "What were the three types of generalizations analyzed in the study?",
    "answer": "The three types of generalizations analyzed were: 1. **Generic generalizations (generics)**: Present tense claims without a quantifier (e.g., 'parental warmth is protective'). 2. **Present tense generalizations**: Converting past tense findings into present tense. 3. **Action guiding generalizations**: Summarizing descriptive statements as recommendations for policy or action.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 50,
    "question": "How did newer LLMs compare to earlier LLMs in terms of overgeneralization tendency?",
    "answer": "Newer LLMs exhibited a **stronger tendency to overgeneralize**. For instance, models like ChatGPT-40 and LLAMA 3.3 70B were associated with the highest proportion of algorithmic overgeneralizations (up to 73%), significantly more than older models.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 51,
    "question": "Did specifically prompting LLMs for accuracy or systematic processing mitigate the overgeneralization bias?",
    "answer": "No. The **systematic prompt** did not significantly change the likelihood of generalized conclusions. The **accuracy prompt** actually **backfired**, leading to LLM summaries being about twice as likely to contain generalized conclusions compared to the simple prompt.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 52,
    "question": "How did LLM summaries compare to human-authored (NEJM JW) summaries of the same scientific articles?",
    "answer": "Overall, LLM summaries (combining models like GPT-4 Turbo and DeepSeek) were **almost five times more likely** to contain generalized conclusions compared to the human-authored NEJM JW summaries (Odds Ratio = 4.85, $p<0.001$). This overgeneralization increased substantially in newer models.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 53,
    "question": "What effect did changing the temperature setting have on the likelihood of generalized conclusions?",
    "answer": "Summaries generated at an LLM temperature of **0** (the most deterministic setting) were **76% less likely to contain generalized conclusions** compared to those generated at temperature 0.7, suggesting temperature control is a potential mitigation strategy.",
    "relevant_papers": [
      "generalization_bias_in_llms"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 54,
    "question": "What is FLEX?",
    "answer": "FLEX (Fairness Benchmark in LLM under Extreme Scenarios) is a new benchmark designed to test whether Large Language Models (LLMs) can sustain fairness and neutrality when exposed to adversarial prompts constructed to induce bias.",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 55,
    "question": "Why was FLEX introduced?",
    "answer": "FLEX was introduced because traditional fairness benchmarks, which assume well-intentioned users, may underestimate the inherent safety risks of LLMs to simple adversarial attacks designed to induce bias.",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 56,
    "question": "What are the three main categories of adversarial variants used to construct the FLEX benchmark?",
    "answer": "The three categories of adversarial variants are: 1. **Persona Injection** (assigning a negative persona). 2. **Competing Objectives** (assigning conflicting tasks like 'Do Anything Now' or refusal suppression). 3. **Text Attack** (introducing non-noticeable manipulations like typos or paraphrasing).",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 57,
    "question": "What key finding was revealed when comparing the accuracy on the source benchmarks ($\\text{Acc}_S$) versus the FLEX benchmark ($\\text{Acc}_F$)?",
    "answer": "$\\text{Acc}_F$ (accuracy in extreme scenarios) was consistently and significantly lower than $\\text{Acc}_S$ (accuracy in common scenarios). This demonstrates that **being unbiased in common scenarios does not guarantee robustness in extreme situations**.",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 58,
    "question": "Which adversarial category was found to be the most effective, especially against open-source models?",
    "answer": "The **Competing Objectives** category, which involves simple, direct instructions, was found to be the most effective and straightforward attack, indicating that even the simplest forms of attack often exploit LLM vulnerabilities.",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 59,
    "question": "How do positive shots and negative shots affect the Attack Success Rate (ASR) in the few-shot setting?",
    "answer": "Positive shots (unbiased demonstrations) generally **decreased ASR**, suggesting a positive outcome. Negative shots (biased demonstrations) generally **amplified the threat by increasing ASR**, as models adhered more strongly to the negative instructions.",
    "relevant_papers": [
      "flex"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 60,
    "question": "What is the primary concern regarding fairness in Retrieval-Augmented Generation (RAG) systems?",
    "answer": "The primary concern is that RAG systems' multi-component architecture makes it challenging to identify and mitigate biases related to sensitive attributes (like gender and location), as utility-driven optimizations often overlook fairness.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 61,
    "question": "What evaluation framework was proposed to assess fairness in RAG methods?",
    "answer": "A systematic fairness evaluation framework tailored for RAG methods was proposed, using a **scenario-based question dataset** focusing on demographic attributes. It evaluates the trade-off between **utility** (Exact Match/ROUGE-1) and **fairness** (Group Disparity/Equalized Odds).",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 62,
    "question": "What is the trade-off between utility and fairness observed in RAG systems?",
    "answer": "A trade-off exists: models optimized for **utility** (high Exact Match scores) typically **do not show corresponding improvements in fairness**, and vice versa, indicating that focusing on accuracy alone can exacerbate biases.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 63,
    "question": "Which RAG component was found to have the most significant influence on both fairness and Exact Match (EM)?",
    "answer": "The **Retriever** component was found to have the most significant influence on both fairness and Exact Match (EM).",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "easy"
  },
  {
    "id": 64,
    "question": "How can increasing the number of retrieved documents impact fairness in RAG?",
    "answer": "For methods like Iter-RetGen and Naive, increasing the number of retrieved documents **significantly improves fairness**. A high initial bias gradually balances out as more documents are retrieved, helping to mitigate gender bias.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 65,
    "question": "What effect did changing the LLM generator size from 8B to 70B parameters have on fairness?",
    "answer": "Increasing the generator size from 8B to 70B parameters caused **bias to fluctuate significantly** (e.g., showing a consistent shift toward a male-favoring bias in the 70B model), while Exact Match remained roughly the same, suggesting a trade-off between model scale and bias direction.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 66,
    "question": "What simple and effective strategy was identified to mitigate bias by modifying the retrieved documents?",
    "answer": "The most straightforward and effective strategy identified was **adjusting the ranking/priority of relevant documents** in the retrieval results. Manually prioritizing relevant documents from the protected group (e.g., female golden documents) substantially mitigated bias while increasing EM.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 67,
    "question": "What specific choices in the experimental setup can lead to conflicting bias results when using text completion to measure biases?",
    "answer": "Conflicting bias results can arise from specific choices regarding the **prompt sets**, **metrics**, **automatic tools** used for evaluation, and the **sampling strategies** employed in the experiments.",
    "relevant_papers": [
      "measuring_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 68,
    "question": "What type of language model capability has led researchers to adopt prompting datasets and text completion as a method for quantifying social biases?",
    "answer": "The capability of some language models to **generate coherent completions** when given a set of textual prompts has led researchers to pose language generation as a way of identifying and quantifying biases vested in pretrained models.",
    "relevant_papers": [
      "measuring_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 69,
    "question": "What is the main finding regarding measuring social biases in open-ended language generation models?",
    "answer": "The practice of measuring biases through text completion is **prone to yielding contradicting results** under different experiment settings, including choices of prompt sets, metrics, automatic tools, and sampling strategies.",
    "relevant_papers": [
      "measuring_bias"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 70,
    "question": "What is the core argument of the paper regarding the feasibility of developing a generally fair LLM?",
    "answer": "The core argument is that the development of a **generally fair Large Language Model is intractable**, or impossible in a rigorous sense. This is due to inherent challenges in technical fairness frameworks that do not logically extend to the general-purpose AI context.",
    "relevant_papers": [
      "impossible_fair_llms"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "hard"
  },
  {
    "id": 71,
    "question": "Why is the \"Fairness Through Unawareness (FTU)\" framework impossible to apply to LLMs by design?",
    "answer": "FTU is impossible for LLMs because they are trained on **massive amounts of unstructured data** where **sensitive attributes are pervasive** and cannot be completely excised. LLMs are readily able to infer personal characteristics from this text, making true unawareness unachievable.",
    "relevant_papers": [
      "impossible_fair_llms"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "hard"
  },
  {
    "id": 72,
    "question": "Why can't a general-purpose LLM be made fair across many different contexts?",
    "answer": "Fairness cannot be guaranteed across many contexts because different populations, use cases, and sensitive attributes impose **different and often conflicting fairness requirements**. An attempt to debias for one context may remove or distort essential information for another context, making general cross-context debiasing infeasible.",
    "relevant_papers": [
      "impossible_fair_llms"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "hard"
  },
  {
    "id": 73,
    "question": "How do LLMs render conventional \"producer-side fairness\" criteria obsolete in multi-sided fairness frameworks?",
    "answer": "LLMs render producer-side fairness obsolete because they can **extract information from producers' content without directing users to the original source**, entirely circumventing the producers and their exposure or benefits.",
    "relevant_papers": [
      "impossible_fair_llms"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 74,
    "question": "Given that general fairness is intractable, what are some promising, tractable future research directions?",
    "answer": "Promising directions include: 1. Crafting **standards for developer responsibility** regarding training data transparency. 2. Refining in-depth methods for **context-specific evaluations**. 3. Building **scalable evaluations** that are iterative and participatory (e.g., using AI capabilities to scale up).",
    "relevant_papers": [
      "impossible_fair_llms"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 75,
    "question": "What unexpected effect does Knowledge Distillation (KD) have on gender bias in smaller models compared to their larger source models?",
    "answer": "After knowledge distillation, the smaller model is found to be **more biased by gender** compared to the source large model, meaning KD amplifies gender bias.",
    "relevant_papers": [
      "knowledge_distillation_amplifies_gender_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 76,
    "question": "What two factors in the Knowledge Distillation process were identified as contributing to gender bias amplification?",
    "answer": "The two contributing factors are: 1. The **limited capacity of the student model** (smaller models show more severe bias). 2. The **cross-entropy loss term** ($\\boldsymbol{L_{ce}}$) between the logit distribution of the student and teacher models.",
    "relevant_papers": [
      "knowledge_distillation_amplifies_gender_bias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 77,
    "question": "What mitigation technique was proposed to reduce gender bias amplification during knowledge distillation, and where was it most effective?",
    "answer": "The technique proposed is applying **mixup** to the knowledge distillation process. It was most effective when applied to the **student's input embedding** and the **teacher's output logit** for gender-related words.",
    "relevant_papers": [
      "knowledge_distillation_amplifies_gender_bias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 78,
    "question": "What is the proposed novel, automated mechanism for debiasing LLMs?",
    "answer": "The proposed mechanism is **automated dataset augmentation** based on the concept of **bias producers** (broad bias creators like ethnicity or sexuality) and **biasers** (specific examples).",
    "relevant_papers": [
      "llm_model_bias_mitigation"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 79,
    "question": "What kind of industries is the automated debiasing mechanism most effective for?",
    "answer": "This approach is most effective for **\"restricted industries\"** (e.g., defense, medical, financial) where data is limited due to confidentiality or availability.",
    "relevant_papers": [
      "llm_model_bias_mitigation"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 80,
    "question": "How does the automated dataset augmentation technique work to mitigate bias?",
    "answer": "The technique involves: 1. Identifying a bias producer/biasers. 2. Iteratively **substituting the biaser** in an entry with other biaser members to create new entries. 3. Applying **content morphism** to each new entry (creating augmented/summarized versions). This is an **autonomous** process, eliminating human annotator bias.",
    "relevant_papers": [
      "llm_model_bias_mitigation"
    ],
    "category": "Mitigation Technique",
    "difficulty": "hard"
  },
  {
    "id": 81,
    "question": "What new metrics were proposed to quantify bias in datasets and models?",
    "answer": "Two new metrics were proposed: the **db-index** (dataset bias index) for quantifying bias in datasets and the **mb-index** (model bias index) for quantifying bias in LLM performance.",
    "relevant_papers": [
      "llm_model_bias_mitigation"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 82,
    "question": "What specific formula and components define the **mb-index** (model bias index)?",
    "answer": "The mb-index is a normalized metric that quantifies bias in an LLM's performance per data entry trained on, defined by the formula: $\\text{mb-index} = \\frac{\\text{Perplexity} \\times \\text{Stereotype Score}}{\\text{Dataset Size}}$. It combines performance (Perplexity) and bias (Stereotype Score) relative to the training data size.",
    "relevant_papers": [
      "llm_model_bias_mitigation"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 83,
    "question": "What primary limitation of simple anti-bias prompts was demonstrated in realistic high-stakes scenarios like hiring?",
    "answer": "Simple anti-bias prompts, effective in controlled settings, were shown to be **fragile and unreliable** when realistic contextual details (like company culture and selective constraints) were introduced, leading to significant race and gender biases (up to 12% differences in interview rates).",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 84,
    "question": "What was the consistent direction of the bias observed when realistic context was introduced?",
    "answer": "The bias consistently favored **Black candidates over White candidates** and **female candidates over male candidates** across all tested models and contexts where realistic context induced bias.",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 85,
    "question": "Did Chain-of-Thought (CoT) monitoring successfully detect the bias observed when realistic context was introduced?",
    "answer": "No. Chain-of-Thought (CoT) monitoring **failed to detect this bias**, as models consistently rationalized biased outcomes with neutral-sounding justifications despite demonstrably biased decisions.",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 86,
    "question": "What internal bias mitigation technique did the study propose?",
    "answer": "The study proposed an internal bias mitigation strategy using **Affine Concept Editing (ACE)**, which identifies and neutralizes sensitive attribute directions (race and gender) within model activations at inference time.",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 87,
    "question": "How robust was the proposed internal bias mitigation technique?",
    "answer": "The technique proved **robust**, consistently reducing measurable bias to very low levels (typically under 1%, always below 2.5%) across all tested models and challenging contextualized settings, while largely maintaining model performance.",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 88,
    "question": "How did LLMs demonstrate the ability to infer demographics, bypassing standard anonymization techniques?",
    "answer": "LLMs demonstrated the ability to infer demographics and become biased from **subtle cues** beyond explicit names or pronouns, such as from **college affiliations** (e.g., HBCUs like Morehouse College).",
    "relevant_papers": [
      "improving_llm_fairness"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 89,
    "question": "What is StereoSet?",
    "answer": "StereoSet is a **large-scale natural dataset in English** containing **16,995 Context Association Tests (CATs)** (triplets) to measure stereotypical biases in pretrained language models.",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 90,
    "question": "What is the primary purpose of StereoSet?",
    "answer": "The primary purpose of StereoSet is to measure stereotypical biases in pretrained language models with respect to **gender, profession, race, and religion**.",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  },
  {
    "id": 91,
    "question": "What two types of Context Association Tests (CATs) are used in StereoSet, and how are they structured?",
    "answer": "The two types of CATs are: 1. **Intrasentence CATs**, which measure bias at the sentence level using a fill-in-the-blank style context. 2. **Intersentence CATs**, which measure bias at the discourse level, providing a context sentence followed by three associative sentences (stereotype, anti-stereotype, unrelated).",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "Bias Evaluation",
    "difficulty": "medium"
  },
  {
    "id": 92,
    "question": "What does the Idealized CAT (ICAT) score measure?",
    "answer": "The ICAT score combines the Language Modeling Score (lms) and the Stereotype Score (ss) into a single metric to measure how close a model is to an idealistic language model.",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 93,
    "question": "What value represents an ideal language model under the ICAT score?",
    "answer": "An **ideal language model** has an **ICAT score of 100**, achieved when its Language Modeling Score (lms) is 100 (always prefers meaningful contexts) and its Stereotype Score (ss) is 50 (shows no stereotypical preference).",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "Bias Evaluation",
    "difficulty": "hard"
  },
  {
    "id": 94,
    "question": "Which family of models exhibited the most idealistic behavior (highest ICAT score) on StereoSet?",
    "answer": "The **GPT2 family of models** exhibited relatively more idealistic behavior and a good balance between the language modeling and stereotype scores compared to models like BERT, RoBERTa, and XLNet.",
    "relevant_papers": [
      "stereoset"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 95,
    "question": "What is the technique of zero-shot self-debiasing?",
    "answer": "Zero-shot self-debiasing is a scalable, prompting-based technique that uses nothing other than the LLM itself to elicit recognition and avoidance of stereotypes with a simple prompt.",
    "relevant_papers": [
      "self_debiasing_llms"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 96,
    "question": "What primary drawback of traditional bias mitigation techniques does zero-shot self-debiasing solve?",
    "answer": "Zero-shot self-debiasing solves the drawback that most traditional techniques require computationally expensive modifications to the training data, model parameters, fine-tuning, or decoding strategy, which may be infeasible without access to a trainable model.",
    "relevant_papers": [
      "self_debiasing_llms"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 97,
    "question": "What were the two approaches proposed for zero-shot self-debiasing?",
    "answer": "The two approaches introduced were: 1. **Self-debiasing via Explanation**, which asks the model to first explain the invalid assumptions in the answer choices before answering. 2. **Self-debiasing via Reprompting**, which asks the LLM to answer a second time with the instruction to \"Remove bias from your answer\".",
    "relevant_papers": [
      "self_debiasing_llms"
    ],
    "category": "Mitigation Technique",
    "difficulty": "easy"
  },
  {
    "id": 98,
    "question": "How did the two self-debiasing techniques compare in reducing stereotypes across the social groups tested?",
    "answer": "Both techniques significantly and consistently reduced stereotyping across the nine social groups tested. The **Reprompting approach** showed **greater reductions in bias** (overall score decreased from 0.136 to 0.023) and was more token-efficient. The **Explanation approach** correctly identified the stereotypical assumptions in the answers and reduced the overall bias score to 0.045.",
    "relevant_papers": [
      "self_debiasing_llms"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 99,
    "question": "What key observation was made about GPT-3.5's responses to occupation-related queries based on gender?",
    "answer": "An invalid inference was observed where the LLM provided conceptually different answers for male and female data engineers, despite the ambiguous query. For a male data engineer, the recommendations were technical, but for a female data engineer, they included non-technical areas (e.g., \"Soft skills\"), reflecting stereotypical assumptions.",
    "relevant_papers": [
      "self_debiasing_llms"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 100,
    "question": "What is prompt brittleness in Large Language Models?",
    "answer": "Prompt brittleness is the high sensitivity of In-Context Learning (ICL) performance to variations in design settings, such as example selection, order, and prompt formatting.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 101,
    "question": "What is the internal cause of prompt brittleness in Large Language Models?",
    "answer": "The internal cause of prompt brittleness is the **inherent bias** in LLMs towards predicting certain answers, which originates from the internal mechanisms of the model.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 102,
    "question": "What specific internal components of an LLM were investigated as causes of bias?",
    "answer": "The specific internal components investigated were **Feedforward Neural Networks (FFNs) vectors** and **attention heads**.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 103,
    "question": "What biases were linked to the investigated internal components of an LLM?",
    "answer": "Biases linked to the internal components include **vanilla label bias** (linked to FFN vectors), and **recency bias** and **selection bias** (linked to attention heads).",
    "relevant_papers": [
      "unibias"
    ],
    "category": "LLM Behavior & Impact",
    "difficulty": "medium"
  },
  {
    "id": 104,
    "question": "What is UniBias?",
    "answer": "UniBias is an **inference-only method** designed to **unveil and eliminate biased components** by directly manipulating the LLM's internal structure.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 105,
    "question": "How does UniBias mitigate bias internally?",
    "answer": "UniBias mitigates bias internally by identifying biased FFN vectors and attention heads based on three criteria (Relatedness, Bias, and Low Variance) and **masking (eliminating)** these biased components during inference.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 106,
    "question": "What were the key experimental results regarding the performance of UniBias?",
    "answer": "UniBias significantly **enhanced ICL performance** and successfully **alleviated prompt brittleness**, achieving stable accuracy with variations consistently less than 4% under various prompt perturbations.",
    "relevant_papers": [
      "unibias"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 107,
    "question": "What are the three primary sources contributing to bias in Large Language Models?",
    "answer": "The three primary sources are: 1. **Training data bias**. 2. **Embedding bias**. 3. **Label bias** (from human annotators or RLHF).",
    "relevant_papers": [
      "taxonomic_survey"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 108,
    "question": "How are metrics for quantifying bias in LLMs categorized in the proposed taxonomy?",
    "answer": "Metrics are categorized based on the data format they utilize: 1. **Embedding-based Metrics**. 2. **Probability-based Metrics**. 3. **Generation-based Metrics**.",
    "relevant_papers": [
      "taxonomic_survey"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 109,
    "question": "What are the four stages used to categorize algorithms for mitigating bias in LLMs?",
    "answer": "Mitigation techniques are structured according to their intervention point in the LLM workflow: 1. **Pre-processing**. 2. **In-training**. 3. **Intra-processing**. 4. **Post-processing**.",
    "relevant_papers": [
      "taxonomic_survey"
    ],
    "category": "Mitigation Technique",
    "difficulty": "easy"
  },
  {
    "id": 110,
    "question": "What is the Challenge regarding Counterfactual Data Augmentation (CDA)?",
    "answer": "The key challenge is **inconsistent data quality**, potentially leading to the generation of unnatural or irrational sentences (e.g., illogical replacement of height/weight associated with one gender to another).",
    "relevant_papers": [
      "taxonomic_survey"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 111,
    "question": "What is a potential future research direction to address the Challenge regarding Counterfactual Data Augmentation (CDA)?",
    "answer": "A potential future research direction is to explore **more rational replacement strategies** or integrate techniques to filter or optimize the generated data to prevent inconsistency and irrational output.",
    "relevant_papers": [
      "taxonomic_survey"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 112,
    "question": "What are the key motivations for the emerging field of LLMs Unlearning?",
    "answer": "The key motivations for LLMs unlearning include compliance with **data privacy regulations** and copyright protection, as well as enabling applications like model detoxification (reducing harmful/toxic content), model correction, and jailbreaking defence.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "easy"
  },
  {
    "id": 113,
    "question": "What new objective is proposed for LLMs unlearning?",
    "answer": "A novel fourth objective, **Robustness**, is proposed, alongside the established goals of Effectiveness, Efficiency, and Utility.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "medium"
  },
  {
    "id": 114,
    "question": "Why is the new objective of Robustness important for LLMs unlearning?",
    "answer": "Robustness is crucial because it ensures the unlearned model can **maintain its intended functionality** in the presence of adversarial inputs, jailbreaking attempts, or other exploitation techniques, thereby preventing unlearned data from resurging.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "medium"
  },
  {
    "id": 115,
    "question": "What are the three main categories of LLMs unlearning algorithms?",
    "answer": "The three main categories of LLMs unlearning algorithms are: 1. **Parameter Modification**. 2. **Input Modification**. 3. **Robust Unlearning**.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "easy"
  },
  {
    "id": 116,
    "question": "What are the major limitations of Input Modification methods (like In-context Unlearning) for achieving fairness?",
    "answer": "Input modification methods are considered unreliable because they offer no formal unlearning guarantee and are vulnerable to adversarial attacks. Crucially, they require storing unwanted data for prompting construction, which contradicts the goal of data privacy established by regulations like GDPR.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "medium"
  },
  {
    "id": 117,
    "question": "Which two threat models are used for robustness evaluations to test if unlearned data can be recovered?",
    "answer": "1. **Membership Inference Attacks (MIA)**, used to determine if a data point was in the training set. 2. **Dynamic Unlearning Attack (DUA)**, a type of adversarial suffix optimisation, and other relearning attacks, used to recover unlearned data.",
    "relevant_papers": [
      "survey_llm_forgetting"
    ],
    "category": "LLM Unlearning",
    "difficulty": "medium"
  },
  {
    "id": 118,
    "question": "What three main taxonomies are used to unify the literature on bias and fairness in LLMs?",
    "answer": "The three main taxonomies are: 1. **Metrics for Bias Evaluation**. 2. **Datasets for Bias Evaluation**. 3. **Techniques for Bias Mitigation**.",
    "relevant_papers": [
      "survey_pdf"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 119,
    "question": "How is Social Bias broadly defined in the context of LLMs?",
    "answer": "Social bias is broadly defined as **disparate treatment or outcomes between social groups** that arise from historical and structural power asymmetries.",
    "relevant_papers": [
      "survey_pdf"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 120,
    "question": "What are the two main categories of harm encompassed by Social Bias?",
    "answer": "The two main categories of harm are **Representational Harms** (e.g., stereotyping, misrepresentation) and **Allocational Harms** (e.g., direct and indirect discrimination).",
    "relevant_papers": [
      "survey_pdf"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 121,
    "question": "What are three core Fairness Desiderata proposed for LLMs to generalize fairness notions beyond simple classification tasks?",
    "answer": "Three core desiderata include: 1. **Invariance** (Definition 9): Output must be identical when the social group in the input is substituted. 2. **Equal Social Group Associations** (Definition 10): A neutral word must be equally likely regardless of the social group mentioned. 3. **Fairness Through Unawareness** (Definition 8): Output must be independent of an explicitly used social group.",
    "relevant_papers": [
      "survey_pdf"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "medium"
  },
  {
    "id": 122,
    "question": "What are the major limitations of Pre-Processing Mitigation techniques, particularly those relying on Data Augmentation?",
    "answer": "Pre-processing techniques, especially data augmentation, are limited by **unscalable and incomplete word lists**, can introduce **factuality or grammatical errors** when swapping terms, and rely on the **tenuous assumption** that social groups are interchangeable, which ignores the underlying complexities of oppression.",
    "relevant_papers": [
      "survey_pdf"
    ],
    "category": "Mitigation Technique",
    "difficulty": "medium"
  },
  {
    "id": 123,
    "question": "What is the primary concern regarding fairness in Retrieval-Augmented Generation (RAG) systems?",
    "answer": "The primary concern is that RAG systems' multi-component architecture makes it challenging to identify and mitigate biases related to sensitive attributes (like gender and location), as utility-driven optimizations often overlook fairness.",
    "relevant_papers": [
      "fairness_in_rag"
    ],
    "category": "Fairness Frameworks",
    "difficulty": "easy"
  },
  {
    "id": 124,
    "question": "What is FLEX?",
    "answer": "FLEX (Fairness Benchmark in LLM under Extreme Scenarios) is a new benchmark designed to test whether Large Language Models (LLMs) can sustain fairness and neutrality when exposed to adversarial prompts constructed to induce bias.",
    "relevant_papers": [
      "flex"
    ],
    "category": "Bias Evaluation",
    "difficulty": "easy"
  }
]